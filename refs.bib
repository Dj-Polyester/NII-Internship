
@article{hogan_knowledge_2022,
	title = {Knowledge {Graphs}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2003.02320},
	doi = {10.1145/3447772},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	language = {en},
	number = {4},
	urldate = {2025-03-09},
	journal = {ACM Computing Surveys},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and Melo, Gerard de and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = may,
	year = {2022},
	note = {arXiv:2003.02320 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
	pages = {1--37},
	annote = {Comment: Revision from v5: Correcting errata from previous version for entailment/models, and some other minor typos},
	file = {PDF:/home/polyester/Zotero/storage/FPA93RWE/Hogan et al. - 2022 - Knowledge Graphs.pdf:application/pdf},
}

@article{pan_unifying_2024,
	title = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}: {A} {Roadmap}},
	volume = {36},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2306.08302},
	doi = {10.1109/TKDE.2024.3352100},
	abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
	language = {en},
	number = {7},
	urldate = {2025-03-09},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
	month = jul,
	year = {2024},
	note = {arXiv:2306.08302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {3580--3599},
	annote = {Comment: A short version of this paper was accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE)},
	file = {PDF:/home/polyester/Zotero/storage/HBBYPI5K/Pan et al. - 2024 - Unifying Large Language Models and Knowledge Graphs A Roadmap.pdf:application/pdf},
}

@misc{shen_exploiting_2020,
	title = {Exploiting {Structured} {Knowledge} in {Text} via {Graph}-{Guided} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2004.14224},
	doi = {10.48550/arXiv.2004.14224},
	abstract = {In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our ﬁrst contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulﬁlled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective which is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efﬁcient than retrieval-based methods that perform entity linking and integration during ﬁnetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on ﬁve benchmark datasets, including question answering and knowledge base completion tasks.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Shen, Tao and Mao, Yi and He, Pengcheng and Long, Guodong and Trischler, Adam and Chen, Weizhu},
	month = apr,
	year = {2020},
	note = {arXiv:2004.14224 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/3VQU8FPB/Shen et al. - 2020 - Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning.pdf:application/pdf},
}

@misc{zhang_e-bert_2021,
	title = {E-{BERT}: {A} {Phrase} and {Product} {Knowledge} {Enhanced} {Language} {Model} for {E}-commerce},
	shorttitle = {E-{BERT}},
	url = {http://arxiv.org/abs/2009.02835},
	doi = {10.48550/arXiv.2009.02835},
	abstract = {Pre-trained language models such as BERT have achieved great success in a broad range of natural language processing tasks. However, BERT cannot well support E-commerce related tasks due to the lack of two levels of domain knowledge, i.e., phrase-level and product-level. On one hand, many E-commerce tasks require accurate understanding of domain phrases, whereas such ﬁne-grained phrase-level knowledge is not explicitly modeled by BERT’s training objective. On the other hand, product-level knowledge like product associations can enhance the language modeling of E-commerce, but they are not factual knowledge thus using them indiscriminately may introduce noise. To tackle the problem, we propose a uniﬁed pre-training framework, namely, E-BERT. Speciﬁcally, to preserve phrase-level knowledge, we introduce Adaptive Hybrid Masking, which allows the model to adaptively switch from learning preliminary word knowledge to learning complex phrases, based on the ﬁtting progress of two modes. To utilize product-level knowledge, we introduce Neighbor Product Reconstruction, which trains E-BERT to predict a product’s associated neighbors with a denoising cross attention layer. Our investigation reveals promising results in four downstream tasks, i.e., review-based question answering, aspect extraction, aspect sentiment classiﬁcation, and product classiﬁcation.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Zhang, Denghui and Yuan, Zixuan and Liu, Yanchi and Zhuang, Fuzhen and Chen, Haifeng and Xiong, Hui},
	month = dec,
	year = {2021},
	note = {arXiv:2009.02835 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/8P3LRBJS/Zhang et al. - 2021 - E-BERT A Phrase and Product Knowledge Enhanced Language Model for E-commerce.pdf:application/pdf},
}

@inproceedings{zhang_ernie_2019,
	address = {Florence, Italy},
	title = {{ERNIE}: {Enhanced} {Language} {Representation} with {Informative} {Entities}},
	shorttitle = {{ERNIE}},
	url = {https://www.aclweb.org/anthology/P19-1139},
	doi = {10.18653/v1/P19-1139},
	abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be ﬁne-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves signiﬁcant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	year = {2019},
	pages = {1441--1451},
	file = {PDF:/home/polyester/Zotero/storage/4TQWZZCJ/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Informative Entities.pdf:application/pdf},
}

@misc{sun_ernie_2021,
	title = {{ERNIE} 3.0: {Large}-scale {Knowledge} {Enhanced} {Pre}-training for {Language} {Understanding} and {Generation}},
	shorttitle = {{ERNIE} 3.0},
	url = {http://arxiv.org/abs/2107.02137},
	doi = {10.48550/arXiv.2107.02137},
	abstract = {Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a uniﬁed framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the ﬁrst place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing the human performance by +0.8\% (90.6\% vs. 89.8\%).},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and Liu, Weixin and Wu, Zhihua and Gong, Weibao and Liang, Jianzhong and Shang, Zhizhou and Sun, Peng and Liu, Wei and Ouyang, Xuan and Yu, Dianhai and Tian, Hao and Wu, Hua and Wang, Haifeng},
	month = jul,
	year = {2021},
	note = {arXiv:2107.02137 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/L2FB7J2H/Sun et al. - 2021 - ERNIE 3.0 Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation.pdf:application/pdf},
}

@misc{wang_knowledge_2022,
	title = {Knowledge {Prompting} in {Pre}-trained {Language} {Model} for {Natural} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2210.08536},
	doi = {10.48550/arXiv.2210.08536},
	abstract = {Knowledge-enhanced Pre-trained Language Model (PLM) has recently received signiﬁcant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of ﬁxed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a knowledge-prompting-based PLM framework KP-PLM. This framework can be ﬂexibly combined with existing mainstream PLMs. Specifically, we ﬁrst construct a knowledge subgraph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub-graph into natural language prompts. To further leverage the factual knowledge from these prompts, we propose two novel knowledge-aware selfsupervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP-PLM over other state-ofthe-art methods in both full-resource and lowresource settings 1.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Wang, Jianing and Huang, Wenkang and Shi, Qiuhui and Wang, Hongbin and Qiu, Minghui and Li, Xiang and Gao, Ming},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08536 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 14 pages, 5 figures. This paper has been accepted for the main conference of EMNLP2022 (long paper)},
	file = {PDF:/home/polyester/Zotero/storage/DXAVD5TA/Wang et al. - 2022 - Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding.pdf:application/pdf},
}

@misc{guu_realm_2020,
	title = {{REALM}: {Retrieval}-{Augmented} {Language} {Model} {Pre}-{Training}},
	shorttitle = {{REALM}},
	url = {http://arxiv.org/abs/2002.08909},
	doi = {10.48550/arXiv.2002.08909},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, ﬁne-tuning and inference. For the ﬁrst time, we show how to pretrain such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by ﬁne-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-theart models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and ﬁnd that we outperform all previous methods by a signiﬁcant margin (4-16\% absolute accuracy), while also providing qualitative beneﬁts such as interpretability and modularity.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	month = feb,
	year = {2020},
	note = {arXiv:2002.08909 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/UGR7TDLT/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Training.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {PDF:/home/polyester/Zotero/storage/XN3BJYAY/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@misc{wilmot_memory_2021,
	title = {Memory and {Knowledge} {Augmented} {Language} {Models} for {Inferring} {Salience} in {Long}-{Form} {Stories}},
	url = {http://arxiv.org/abs/2109.03754},
	doi = {10.48550/arXiv.2109.03754},
	abstract = {Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapteraligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Wilmot, David and Keller, Frank},
	month = sep,
	year = {2021},
	note = {arXiv:2109.03754 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted to the EMNLP 2021 Conference as a long-paper, 9 pages, 15 pages with appendices and references, 2 figures, 4 tables},
	file = {PDF:/home/polyester/Zotero/storage/RR6FNRDE/Wilmot and Keller - 2021 - Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories.pdf:application/pdf},
}

@misc{wu_efficient_2022,
	title = {An {Efficient} {Memory}-{Augmented} {Transformer} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2210.16773},
	doi = {10.48550/arXiv.2210.16773},
	abstract = {Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) -- it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 -{\textgreater} 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5. Our code and datasets are available at https://github. com/uclnlp/EMAT.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = oct,
	year = {2022},
	note = {arXiv:2210.16773 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2022 main conference long paper. 8 pages, 6 figures},
	file = {PDF:/home/polyester/Zotero/storage/WZQ63G66/Wu et al. - 2022 - An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@misc{karaca_ai-book_2024,
	title = {{AI}-{Book}},
	url = {https://github.com/Dj-Polyester/AI-Book},
	urldate = {2025-03-10},
	author = {Karaca, Batuhan},
	month = may,
	year = {2024},
	file = {GitHub - Dj-Polyester/AI-Book\: Repository for the files of the book:/home/polyester/Zotero/storage/WD53F9XK/AI-Book.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short} {Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf},
	number = {8},
	urldate = {2025-03-11},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
	year = {1997},
	pages = {1735--1780},
	file = {PDF:/home/polyester/Zotero/storage/ZG8HZAT6/LSTM.pdf:application/pdf},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {PDF:/home/polyester/Zotero/storage/4A5N4UM8/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf},
}

@misc{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	doi = {10.48550/arXiv.1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = mar,
	year = {2018},
	note = {arXiv:1802.05365 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	file = {PDF:/home/polyester/Zotero/storage/MP7FSYW6/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf},
}

@misc{parikh_decomposable_2016,
	title = {A {Decomposable} {Attention} {Model} for {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/1606.01933},
	doi = {10.48550/arXiv.1606.01933},
	abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	month = sep,
	year = {2016},
	note = {arXiv:1606.01933 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages, 1 figure, Proceeedings of EMNLP 2016},
	file = {PDF:/home/polyester/Zotero/storage/T98M78Q5/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Language Inference.pdf:application/pdf},
}

@misc{seo_bidirectional_2018,
	title = {Bidirectional {Attention} {Flow} for {Machine} {Comprehension}},
	url = {http://arxiv.org/abs/1611.01603},
	doi = {10.48550/arXiv.1611.01603},
	abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a ﬁxed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention ﬂow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
	month = jun,
	year = {2018},
	note = {arXiv:1611.01603 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {PDF:/home/polyester/Zotero/storage/CAWAY45W/Seo et al. - 2018 - Bidirectional Attention Flow for Machine Comprehension.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {PDF:/home/polyester/Zotero/storage/FKRH9DVZ/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/L39IHNS4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@misc{alammar_illustrated_2018,
	title = {The {Illustrated} {BERT}, {ELMo}, and co. ({How} {NLP} {Cracked} {Transfer} {Learning}) – {Jay} {Alammar} – {Visualizing} machine learning one concept at a time.},
	url = {https://jalammar.github.io/illustrated-bert/},
	urldate = {2025-03-11},
	author = {Alammar, Jay},
	month = dec,
	year = {2018},
	file = {The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.:/home/polyester/Zotero/storage/89REZMAG/illustrated-bert.html:text/html},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	file = {PDF:/home/polyester/Zotero/storage/D5PBQ3AN/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {PDF:/home/polyester/Zotero/storage/VM7KDILE/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{schick_exploiting_2021,
	title = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/2001.07676},
	doi = {10.48550/arXiv.2001.07676},
	abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Schick, Timo and Schütze, Hinrich},
	month = jan,
	year = {2021},
	note = {arXiv:2001.07676 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at EACL2021},
	file = {PDF:/home/polyester/Zotero/storage/MEX8KWK7/Schick and Schütze - 2021 - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.pdf:application/pdf},
}

@misc{zhu_llms_2024,
	title = {{LLMs} for {Knowledge} {Graph} {Construction} and {Reasoning}: {Recent} {Capabilities} and {Future} {Opportunities}},
	shorttitle = {{LLMs} for {Knowledge} {Graph} {Construction} and {Reasoning}},
	url = {http://arxiv.org/abs/2305.13168},
	doi = {10.48550/arXiv.2305.13168},
	abstract = {This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.},
	language = {en},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
	month = dec,
	year = {2024},
	note = {arXiv:2305.13168 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: World Wide Web Journal},
	file = {PDF:/home/polyester/Zotero/storage/BBME6XGR/Zhu et al. - 2024 - LLMs for Knowledge Graph Construction and Reasoning Recent Capabilities and Future Opportunities.pdf:application/pdf},
}

@misc{leung_micro_2022,
	title = {Micro, {Macro} \& {Weighted} {Averages} of {F1} {Score}, {Clearly} {Explained}},
	url = {https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f/},
	urldate = {2025-03-13},
	author = {Leung, Kenneth},
	month = jan,
	year = {2022},
	file = {Micro, Macro & Weighted Averages of F1 Score, Clearly Explained | Towards Data Science:/home/polyester/Zotero/storage/SPV6BH4N/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f.html:text/html},
}

@misc{stoica_re-tacred_2021,
	title = {Re-{TACRED}: {Addressing} {Shortcomings} of the {TACRED} {Dataset}},
	shorttitle = {Re-{TACRED}},
	url = {http://arxiv.org/abs/2104.08398},
	doi = {10.48550/arXiv.2104.08398},
	abstract = {TACRED is one of the largest and most widely used sentencelevel relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-theart performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50\% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8\% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its ﬁndings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After veriﬁcation, we observed that 23.9\% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3\% and helps uncover signiﬁcant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.},
	language = {en},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Stoica, George and Platanios, Emmanouil Antonios and Póczos, Barnabás},
	month = apr,
	year = {2021},
	note = {arXiv:2104.08398 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/7FV2XNG7/Stoica et al. - 2021 - Re-TACRED Addressing Shortcomings of the TACRED Dataset.pdf:application/pdf},
}

@book{lapan_deep_2020,
	address = {Place of publication not identified},
	edition = {2nd ed},
	title = {Deep reinforcement learning hands-on: apply modern {RL} methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, {Second} edition},
	isbn = {978-1-83882-699-4 978-1-83882-004-6},
	shorttitle = {Deep reinforcement learning hands-on},
	abstract = {New edition of the bestselling guide to deep reinforcement learning and how it's used to solve complex real-world problems. Revised and expanded to include multi-agent methods, discrete optimization, RL in robotics, advanced exploration techniques, and more Key Features Second edition of the bestselling introduction to deep reinforcement learning, expanded with six new chapters Learn advanced exploration techniques including noisy networks, pseudo-count, and network distillation methods Apply RL methods to cheap hardware robotics platforms Book Description Deep Reinforcement Learning Hands-On, Second Edition is an updated and expanded version of the bestselling guide to the very latest reinforcement learning (RL) tools and techniques. It provides you with an introduction to the fundamentals of RL, along with the hands-on ability to code intelligent learning agents to perform a range of practical tasks. With six new chapters devoted to a variety of up-to-the-minute developments in RL, including discrete optimization (solving the Rubik's Cube), multi-agent methods, Microsoft's TextWorld environment, advanced exploration techniques, and more, you will come away from this book with a deep understanding of the latest innovations in this emerging field. In addition, you will gain actionable insights into such topic areas as deep Q-networks, policy gradient methods, continuous control problems, and highly scalable, non-gradient methods. You will also discover how to build a real hardware robot trained with RL for less than},
	language = {en},
	publisher = {Packt Publishing},
	editor = {Lapan, Maxim},
	year = {2020},
	file = {PDF:/home/polyester/Zotero/storage/79SLGZLZ/Lapan - 2020 - Deep reinforcement learning hands-on apply modern RL methods to practical problems of chatbots, rob.pdf:application/pdf},
}

@misc{shao_deepseekmath_2024,
	title = {{DeepSeekMath}: {Pushing} the {Limits} of {Mathematical} {Reasoning} in {Open} {Language} {Models}},
	shorttitle = {{DeepSeekMath}},
	url = {http://arxiv.org/abs/2402.03300},
	doi = {10.48550/arXiv.2402.03300},
	abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
	language = {en},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
	month = apr,
	year = {2024},
	note = {arXiv:2402.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/HAP6PGBU/Shao et al. - 2024 - DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf:application/pdf},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	language = {en},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/ZPL2T729/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/polyester/Zotero/storage/H32TK78M/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{weng_policy_2018,
	title = {Policy {Gradient} {Algorithms}},
	url = {https://lilianweng.github.io/posts/2018-04-08-policy-gradient/},
	urldate = {2025-03-16},
	author = {Weng, Lilian},
	month = apr,
	year = {2018},
	file = {Policy Gradient Algorithms | Lil'Log:/home/polyester/Zotero/storage/6UC66DY3/2018-04-08-policy-gradient.html:text/html},
}

@misc{rennie_self-critical_2017,
	title = {Self-critical {Sequence} {Training} for {Image} {Captioning}},
	url = {http://arxiv.org/abs/1612.00563},
	doi = {10.48550/arXiv.1612.00563},
	abstract = {Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep endto-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, signiﬁcant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a “baseline” to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we ﬁnd that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jarret and Goel, Vaibhava},
	month = nov,
	year = {2017},
	note = {arXiv:1612.00563 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017 + additional analysis + fixed baseline results, 16 pages},
	file = {PDF:/home/polyester/Zotero/storage/3TSTBR9T/Rennie et al. - 2017 - Self-critical Sequence Training for Image Captioning.pdf:application/pdf},
}

@inproceedings{zhang_position-aware_2017,
	address = {Copenhagen, Denmark},
	title = {Position-aware {Attention} and {Supervised} {Data} {Improve} {Slot} {Filling}},
	url = {http://aclweb.org/anthology/D17-1004},
	doi = {10.18653/v1/D17-1004},
	abstract = {Organized relational knowledge in the form of “knowledge graphs” is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We ﬁrst propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot ﬁlling system, its F1 score increases markedly from 22.2\% to 26.7\%.},
	language = {en},
	urldate = {2025-03-16},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
	year = {2017},
	pages = {35--45},
	file = {PDF:/home/polyester/Zotero/storage/GBVAECVF/Zhang et al. - 2017 - Position-aware Attention and Supervised Data Improve Slot Filling.pdf:application/pdf},
}

@misc{carta_iterative_2023,
	title = {Iterative {Zero}-{Shot} {LLM} {Prompting} for {Knowledge} {Graph} {Construction}},
	url = {http://arxiv.org/abs/2307.01128},
	doi = {10.48550/arXiv.2307.01128},
	abstract = {In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for “guiding” the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Carta, Salvatore and Giuliani, Alessandro and Piano, Leonardo and Podda, Alessandro Sebastian and Pompianu, Livio and Tiddia, Sandro Gabriele},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01128 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/9W5B2L9Q/Carta et al. - 2023 - Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction.pdf:application/pdf},
}

@misc{noauthor_tacred_nodate,
	title = {{TACRED} {Benchmark} ({Relation} {Extraction}) {\textbar} {Papers} {With} {Code}},
	url = {https://paperswithcode.com/sota/relation-extraction-on-tacred},
	urldate = {2025-03-16},
	file = {TACRED Benchmark (Relation Extraction) | Papers With Code:/home/polyester/Zotero/storage/WBYRUVSA/relation-extraction-on-tacred.html:text/html},
}

@misc{abdin_phi-3_2024,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	doi = {10.48550/arXiv.2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\%, 78\% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Martin and Cai, Qin and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng, Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao, Mei and Gao, Min and Garg, Amit and Giorno, Allie Del and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Hu, Wenxiang and Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and Kim, Young Jin and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu, Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh, Ali and Majercak, David and Mazzola, Matt and Mendes, Caio César Teodoro and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Ren, Liliang and Rosa, Gustavo de and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang, Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin and Wang, Yu and Ward, Rachel and Wen, Wen and Witte, Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
	month = aug,
	year = {2024},
	note = {arXiv:2404.14219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 24 pages},
	file = {PDF:/home/polyester/Zotero/storage/5EVBBP6R/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language Model Locally on Your Phone.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {PDF:/home/polyester/Zotero/storage/629AW4QJ/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{kalajdzievski_rank_2023,
	title = {A {Rank} {Stabilization} {Scaling} {Factor} for {Fine}-{Tuning} with {LoRA}},
	url = {http://arxiv.org/abs/2312.03732},
	doi = {10.48550/arXiv.2312.03732},
	abstract = {As large language models (LLMs) have become increasingly compute and memory intensive, parameter-efficient fine-tuning (PEFT) methods are now a common strategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA), which adds trainable low-rank “adapters” to selected layers. Each adapter consists of a low-rank matrix product, multiplicatively scaled by a rank-dependent factor. This scaling factor, which divides adapters by a factor of the rank, results in slowed learning and stunted performance for LoRA with higher-rank adapters. Consequently, the use of LoRA in practice has generally been limited to very low ranks. In this work, we study the impact of the scaling factor on the learning process and prove that LoRA adapters should be divided by a factor of the square root of the rank. Modifying LoRA with the appropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA) method, easily provides for a fine-tuning compute/performance trade-off, where larger ranks can be used to trade off increased computational resources during training for better fine-tuning performance, with no change in inference computing cost.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Kalajdzievski, Damjan},
	month = nov,
	year = {2023},
	note = {arXiv:2312.03732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/QBP27NPD/Kalajdzievski - 2023 - A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA.pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	doi = {10.48550/arXiv.2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	file = {PDF:/home/polyester/Zotero/storage/PYRR4AA9/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}

@misc{dettmers_8-bit_2022,
	title = {8-bit {Optimizers} via {Block}-wise {Quantization}},
	url = {http://arxiv.org/abs/2110.02861},
	doi = {10.48550/arXiv.2110.02861},
	abstract = {Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the ﬁrst optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE ﬁnetuning, ImageNet classiﬁcation, WMT’14 machine translation, MoCo v2 contrastive ImageNet pretraining+ﬁnetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-sourceour 8-bit optimizers as a drop-in replacement that only requires a two-line code change.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
	month = jun,
	year = {2022},
	note = {arXiv:2110.02861 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR2022 spotlight version},
	file = {PDF:/home/polyester/Zotero/storage/MIHCQV9H/Dettmers et al. - 2022 - 8-bit Optimizers via Block-wise Quantization.pdf:application/pdf},
}

@misc{li_loftq_2023,
	title = {{LoftQ}: {LoRA}-{Fine}-{Tuning}-{Aware} {Quantization} for {Large} {Language} {Models}},
	shorttitle = {{LoftQ}},
	url = {http://arxiv.org/abs/2310.08659},
	doi = {10.48550/arXiv.2310.08659},
	abstract = {Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pretrained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. The code is available on https://github.com/yxli2123/LoftQ.},
	language = {en},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Li, Yixiao and Yu, Yifan and Liang, Chen and He, Pengcheng and Karampatziakis, Nikos and Chen, Weizhu and Zhao, Tuo},
	month = nov,
	year = {2023},
	note = {arXiv:2310.08659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/XKTMBIT5/Li et al. - 2023 - LoftQ LoRA-Fine-Tuning-Aware Quantization for Large Language Models.pdf:application/pdf},
}

@misc{grobbelaar_pygmalionaipygmalion-7b_2023,
	title = {{PygmalionAI}/pygmalion-7b · {Reasoning} behind bfloat instead of float?},
	url = {https://huggingface.co/PygmalionAI/pygmalion-7b/discussions/3},
	urldate = {2025-03-16},
	author = {Grobbelaar, Francois},
	month = may,
	year = {2023},
	file = {PygmalionAI/pygmalion-7b · Reasoning behind bfloat instead of float?:/home/polyester/Zotero/storage/4GRL92KN/3.html:text/html},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, US},
	title = {{BLEU}: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	url = {https://aclanthology.org/P02-1040.pdf},
	urldate = {2025-03-16},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	pages = {311--318},
	file = {PDF:/home/polyester/Zotero/storage/TTCKUAUF/P02-1040.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	title = {Rouge: {A} package for automatic evaluation of summaries},
	url = {https://aclanthology.org/W04-1013.pdf},
	urldate = {2025-03-16},
	booktitle = {Text summarization branches out},
	author = {Lin, Chin-Yew},
	year = {2004},
	pages = {74--81},
	file = {PDF:/home/polyester/Zotero/storage/LJY5TT9N/W04-1013.pdf:application/pdf},
}

@misc{microsoft_microsoftphi-3-mini-128k-instruct_nodate,
	title = {microsoft/{Phi}-3-mini-128k-instruct · {Hugging} {Face}},
	url = {https://huggingface.co/microsoft/Phi-3-mini-128k-instruct},
	urldate = {2025-03-18},
	author = {Microsoft},
	file = {microsoft/Phi-3-mini-128k-instruct · Hugging Face:/home/polyester/Zotero/storage/JX2CJFCQ/Phi-3-mini-128k-instruct.html:text/html},
}

@inproceedings{banarjee_meteor_2005,
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	url = {https://www.cs.cmu.edu/~alavie/papers/BanerjeeLavie2005-final.pdf},
	urldate = {2025-03-18},
	booktitle = {Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
	author = {Banarjee, Satanjeev and Lavie, Alon},
	year = {2005},
	pages = {65--57},
	file = {PDF:/home/polyester/Zotero/storage/IUI6SM4W/BanerjeeLavie2005-final.pdf:application/pdf},
}

@misc{muller_when_2020,
	title = {When {Does} {Label} {Smoothing} {Help}?},
	url = {http://arxiv.org/abs/1906.02629},
	doi = {10.48550/arXiv.1906.02629},
	abstract = {The generalization and learning speed of a multi-class neural network can often be signiﬁcantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-conﬁdent and label smoothing has been used in many state-of-the-art models, including image classiﬁcation, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can signiﬁcantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model’s predictions.},
	language = {en},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:1906.02629 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at NeurIPS 2019, corrected mutual information formulas},
	file = {PDF:/home/polyester/Zotero/storage/7X2DQU4K/Müller et al. - 2020 - When Does Label Smoothing Help.pdf:application/pdf},
}

@misc{watanabe_tree-structured_2023,
	title = {Tree-{Structured} {Parzen} {Estimator}: {Understanding} {Its} {Algorithm} {Components} and {Their} {Roles} for {Better} {Empirical} {Performance}},
	shorttitle = {Tree-{Structured} {Parzen} {Estimator}},
	url = {http://arxiv.org/abs/2304.11127},
	doi = {10.48550/arXiv.2304.11127},
	abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
	language = {en},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Watanabe, Shuhei},
	month = may,
	year = {2023},
	note = {arXiv:2304.11127 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/home/polyester/Zotero/storage/44D4BIHG/Watanabe - 2023 - Tree-Structured Parzen Estimator Understanding Its Algorithm Components and Their Roles for Better.pdf:application/pdf},
}

@misc{dao-ailab_github_2022,
	title = {{GitHub} - {Dao}-{AILab}/flash-attention at 27f501dbe011f4371bff938fe7e09311ab3002fa},
	url = {https://github.com/Dao-AILab/flash-attention/tree/27f501dbe011f4371bff938fe7e09311ab3002fa},
	urldate = {2025-03-18},
	author = {Dao-AILab},
	month = may,
	year = {2022},
	file = {GitHub - Dao-AILab/flash-attention at 27f501dbe011f4371bff938fe7e09311ab3002fa:/home/polyester/Zotero/storage/XUEHRAJ9/27f501dbe011f4371bff938fe7e09311ab3002fa.html:text/html},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	language = {en},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {PDF:/home/polyester/Zotero/storage/UNUFQRAS/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	doi = {10.48550/arXiv.1904.09751},
	abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops.},
	language = {en},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published in ICLR 2020},
	file = {PDF:/home/polyester/Zotero/storage/C9MJUV2Y/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:application/pdf},
}

@inproceedings{freitag_beam_2017,
	title = {Beam {Search} {Strategies} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1702.01806},
	doi = {10.18653/v1/W17-3207},
	abstract = {The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-toright beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-toright while keeping a ﬁxed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more ﬂexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43\% for the two language pairs German→English and Chinese→English without losing any translation quality.},
	language = {en},
	urldate = {2025-03-18},
	booktitle = {Proceedings of the {First} {Workshop} on {Neural} {Machine} {Translation}},
	author = {Freitag, Markus and Al-Onaizan, Yaser},
	year = {2017},
	note = {arXiv:1702.01806 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {56--60},
	annote = {Comment: First Workshop on Neural Machine Translation, 2017},
	file = {PDF:/home/polyester/Zotero/storage/I3342N2A/Freitag and Al-Onaizan - 2017 - Beam Search Strategies for Neural Machine Translation.pdf:application/pdf},
}

@misc{pytorch_contributors_torchutilscheckpoint_2024,
	title = {torch.utils.checkpoint — {PyTorch} 2.6 documentation},
	url = {https://pytorch.org/docs/stable/checkpoint.html},
	urldate = {2025-03-21},
	author = {PyTorch Contributors},
	year = {2024},
	file = {torch.utils.checkpoint — PyTorch 2.6 documentation:/home/polyester/Zotero/storage/NUY7DYZM/checkpoint.html:text/html},
}

@inproceedings{noauthor_notitle_nodate,
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {PDF:/home/polyester/Zotero/storage/JI46WE7D/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	language = {en},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/polyester/Zotero/storage/2JR7DA26/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf},
}
