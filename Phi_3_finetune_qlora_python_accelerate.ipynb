{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZAC8jw6hYza"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq --upgrade optuna transformers==4.47 bitsandbytes peft accelerate datasets nvidia-ml-py3 matplotlib torchmetrics tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug530umUt-hZ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils Cell"
      ],
      "metadata": {
        "id": "Agjjp7QfDnWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omApdqbQaTWy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import sys\n",
        "from typing import Iterable, Mapping, Union, Callable, Optional, List, Dict, Any\n",
        "from types import NoneType\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers.utils import logging\n",
        "from huggingface_hub.utils import enable_progress_bars, disable_progress_bars\n",
        "from dataclasses import dataclass, field\n",
        "import time\n",
        "import os\n",
        "\n",
        "Numeric = Union[int, float, torch.Tensor]\n",
        "class Identity():\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return Identity()\n",
        "  def __getattribute__(self, name):\n",
        "    return Identity()\n",
        "class Logger:\n",
        "  def __init__(\n",
        "      self,\n",
        "      verbosity=logging.INFO,\n",
        "      enable_progress_bars=True,\n",
        "      handle_enabled = True,\n",
        "    ):\n",
        "    self.set_verbosity(verbosity)\n",
        "    self.set_progress_bars(enable_progress_bars)\n",
        "    self.handle_enabled = handle_enabled\n",
        "  def set_handle(self, handle=None, enable=None):\n",
        "    if enable is not None:\n",
        "      self.handle_enabled = enable\n",
        "    if handle != None:\n",
        "      self.handle = handle\n",
        "  def set_verbosity(self, verbosity):\n",
        "    self.verbosity = verbosity\n",
        "  def set_progress_bars(self, enable):\n",
        "    if enable:\n",
        "      enable_progress_bars()\n",
        "    else:\n",
        "      disable_progress_bars()\n",
        "  def is_verbosity(self, verbosity):\n",
        "    return self.verbosity <= verbosity\n",
        "  def log(self, verbosity, *args, **kwargs):\n",
        "    if self.is_verbosity(verbosity):\n",
        "      print(*args, **kwargs)\n",
        "  def critical(self, *args, **kwargs):\n",
        "    self.log(logging.CRITICAL, *args, **kwargs)\n",
        "  def error(self, *args, **kwargs):\n",
        "    self.log(logging.ERROR, *args, **kwargs)\n",
        "  def warning(self, *args, **kwargs):\n",
        "    self.log(logging.WARNING, *args, **kwargs)\n",
        "  def info(self, *args, **kwargs):\n",
        "    self.log(logging.INFO, *args, **kwargs)\n",
        "  def debug(self, *args, **kwargs):\n",
        "    self.log(logging.DEBUG, *args, **kwargs)\n",
        "  def __getattr__(self, name):\n",
        "    try:\n",
        "      obj = self.handle\n",
        "    except AttributeError:\n",
        "      obj = super()\n",
        "    return obj.__getattribute__(name) if self.handle_enabled else Identity()\n",
        "# logging.CRITICAL or logging.FATAL: only report the most critical errors.\n",
        "# logging.ERROR: only report errors.\n",
        "# logging.WARNING or logging.WARN: only reports error and warnings. This is the default level used by the library.\n",
        "# logging.INFO: reports error, warnings and basic information.\n",
        "# logging.DEBUG: report all information.\n",
        "# GENERAL\n",
        "def exists(obj, attr):\n",
        "  return hasattr(obj, attr) and getattr(obj, attr) != None\n",
        "def notexists(obj, attr):\n",
        "  return not hasattr(obj, attr) or getattr(obj, attr) == None\n",
        "# DATASET DEBUG\n",
        "def is_double_iter(data):\n",
        "  return isinstance(data, Iterable) and all(isinstance(inner, Iterable) for inner in data)\n",
        "def is_good_for_lens(data):\n",
        "  return isinstance(data, Iterable) and all(isinstance(inner, Iterable) and getattr(inner, \"__len__\", None) is not None for inner in data)\n",
        "def check_pretty_print_list_of_lists(data):\n",
        "  if is_double_iter(data):\n",
        "    logger.info(\"[\")\n",
        "    for inner in data:\n",
        "        logger.info(f\"  {repr(inner)},\")\n",
        "    logger.info(\"]\")\n",
        "  else:\n",
        "    logger.info(data)\n",
        "def pretty_print_dict(data):\n",
        "  logger.info(\"[\")\n",
        "  for key, value in data.items():\n",
        "      logger.info(f\"{key}: \", end=\"\")\n",
        "      check_pretty_print_list_of_lists(value)\n",
        "  logger.info(\"}\")\n",
        "def pretty_print(data):\n",
        "  if isinstance(data, Mapping):\n",
        "    pretty_print_dict(data)\n",
        "  else:\n",
        "    check_pretty_print_list_of_lists(data)\n",
        "def detokenized(\n",
        "    data,\n",
        "    tokenizer,\n",
        "    dontconvert=[\"attention_mask\"],\n",
        "    exclude=[\"labels\"],\n",
        "    ):\n",
        "  if isinstance(data, Mapping):\n",
        "    tmpdic = {}\n",
        "    for k,v in data.items():\n",
        "      if k in dontconvert:\n",
        "        tmp = v\n",
        "      elif isinstance(v, Iterable):\n",
        "        tmp = detokenized(v,tokenizer)\n",
        "      else:\n",
        "        raise TypeError()\n",
        "      if k not in exclude:\n",
        "        tmpdic[k] = tmp\n",
        "    return tmpdic\n",
        "  if isinstance(data, Iterable):\n",
        "    tmpobj = None\n",
        "    if all(isinstance(inner, Iterable) for inner in data):\n",
        "      if all(isinstance(innermost, Numeric) for inner in data for innermost in inner):\n",
        "        if any(innermost < 0 for inner in data for innermost in inner):\n",
        "          tmpobj = data\n",
        "        else:\n",
        "          tmpobj = list(map(tokenizer.convert_ids_to_tokens, data))\n",
        "    if tmpobj != None and not any(isinstance(inner, str) for inner in data):\n",
        "      return tmpobj\n",
        "    elif tmpobj != None:\n",
        "      return type(data)(tmpobj)\n",
        "    else:\n",
        "      raise TypeError(f\"Unknown nested type: {type(next(data))}\")\n",
        "  else:\n",
        "    raise TypeError(f\"Unknown type: {type(data)}\")\n",
        "def print_detokenized(\n",
        "    data,\n",
        "    tokenizer,\n",
        "    dontconvert=[\"attention_mask\"],\n",
        "    exclude=[\"labels\"],\n",
        "    ):\n",
        "  pretty_print(detokenized(data, tokenizer, dontconvert, exclude))\n",
        "def lens(data):\n",
        "  return [len(a) for a in data]\n",
        "def type_shape(data_dic):\n",
        "  return {k:(v.dtype, v.shape, v.device) if isinstance(v,torch.Tensor) else ((type(v), lens(v)) if is_good_for_lens(v) else type(v)) for k,v in data_dic.items()}\n",
        "def get_random_or_indexed(dataset, index=None, split=\"train\"):\n",
        "  tmpsplit = dataset[split]\n",
        "  i = index if isinstance(index, int) else random.randrange(len(tmpsplit))\n",
        "  return i, tmpsplit[i]\n",
        "\n",
        "# SIZE DEBUG\n",
        "def get_bit_scaling_from_type(_type=None):\n",
        "  types = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
        "  if _type == \"b\":\n",
        "    return 8\n",
        "  elif _type in types:\n",
        "    return 1024**(-types.index(_type))\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown type: {_type}\")\n",
        "def get_readable_size(size_bits, size_type_to_print=None, join=True):\n",
        "  if size_type_to_print == None:\n",
        "    size_type_to_print = \"b\"\n",
        "  size_scaling = get_bit_scaling_from_type(size_type_to_print)\n",
        "  _size = size_bits * size_scaling\n",
        "  return f\"{_size} {size_type_to_print}\" if join else (_size, size_type_to_print)\n",
        "def get_obj_size(obj, gpu_only=False):\n",
        "  obj_handle = None\n",
        "  if isinstance(obj, torch.Tensor):\n",
        "    if (not gpu_only or (gpu_only and obj.is_cuda)):\n",
        "      obj_handle = obj.storage()\n",
        "  elif not gpu_only:\n",
        "    obj_handle = obj\n",
        "  obj_size = 0\n",
        "  if obj_handle is not None:\n",
        "    obj_size = sys.getsizeof(obj_handle)\n",
        "  return obj_size\n",
        "def get_obj_size_rec(obj, gpu_only=False, depth = None):\n",
        "  marked = {id(obj)}\n",
        "  obj_q = [obj]\n",
        "  sz = 0\n",
        "  while obj_q:\n",
        "      sz += sum([get_obj_size(obj, gpu_only) for obj in obj_q])\n",
        "      new_refr = {}\n",
        "\n",
        "      for o in gc.get_referents(*obj_q):\n",
        "        o_id = id(o)\n",
        "        if o_id not in marked and not isinstance(o, type):\n",
        "          new_refr[o_id] = o\n",
        "      \"\"\"\n",
        "      all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))\n",
        "      new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}\n",
        "      \"\"\"\n",
        "      obj_q = new_refr.values()\n",
        "      marked.update(new_refr.keys())\n",
        "      if depth is not None:\n",
        "        depth-=1\n",
        "        if depth <= 0:\n",
        "          break\n",
        "  return sz\n",
        "def get_readable_obj_size(\n",
        "    obj,\n",
        "    gpu_only=False,\n",
        "    depth = None,\n",
        "    size_type_to_print=None,\n",
        "    join=False,\n",
        "    ):\n",
        "  obj_size = get_obj_size_rec(obj, gpu_only, depth)\n",
        "  return get_readable_size(obj_size, size_type_to_print, join)\n",
        "def get_objs_size(\n",
        "    gpu_only=False,\n",
        "    depth = None,\n",
        "    size_type_to_print=None,\n",
        "    increasing = False,\n",
        "    old_obj_size = {}\n",
        "    ):\n",
        "  objs_size = {}\n",
        "  for obj in gc.get_objects():\n",
        "    obj_id = id(obj)\n",
        "    obj_type = type(obj)\n",
        "    if obj_id not in old_obj_size:\n",
        "      obj_size = get_readable_obj_size(\n",
        "          obj,\n",
        "          gpu_only,\n",
        "          depth,\n",
        "          size_type_to_print\n",
        "      )\n",
        "      if obj_size[0] > 0:\n",
        "        objs_size[obj_id] = (obj_type,)+obj_size\n",
        "      elif obj_size[0] < 0:\n",
        "        raise ValueError(f\"Negative object size for {obj_size}\")\n",
        "  objs_size = dict(sorted(objs_size.items(), key=lambda item: item[1][1], reverse=not increasing))\n",
        "  return {k: \" \".join([str(v[0]), str(v[1]), v[2]]) for k,v in objs_size.items()}\n",
        "\n",
        "#PARAM DEBUG\n",
        "def get_info_from_param(param):\n",
        "  tmp = {\n",
        "      \"device\": param.device,\n",
        "      \"dtype\": param.dtype,\n",
        "      \"shape\": param.shape,\n",
        "      \"numel\": param.numel(),\n",
        "      \"size\": sys.getsizeof(param.storage()),\n",
        "      \"requires_grad\": param.requires_grad,\n",
        "  }\n",
        "  if exists(param, \"grad\"):\n",
        "    tmp[\"grad dtype\"] = param.grad.dtype,\n",
        "    tmp[\"grad mean\"] = param.grad.mean(),\n",
        "  return tmp\n",
        "def info_str_param(name, param_info, size_type_to_print):\n",
        "  return f\"{name}: \"+\", \".join([f\"{k}={get_readable_size(v, size_type_to_print)}\" if k==\"size\" else f\"{k}={v}\" for k,v in param_info.items()])\n",
        "class BinnedCounter():\n",
        "    def __init__(self, bins=None, right=False, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        right=True <= <\n",
        "        right=False < <=\n",
        "        \"\"\"\n",
        "        self.right = right\n",
        "        self.device = device\n",
        "        self.bins, self._bin_diff = self.create_bins(bins)\n",
        "        self.length = len(self.bins)+1\n",
        "        self._counts = torch.zeros(self.length).long().to(self.device)\n",
        "    def create_bins(self, binargs):\n",
        "        if isinstance(binargs, (list, tuple)):\n",
        "          if len(binargs) != 3:\n",
        "            raise ValueError(f\"binargs has the wrong length {len(binargs)}\")\n",
        "          num_bins = int(binargs[-1])\n",
        "          start, end = tuple(map(float, binargs[:-1]))\n",
        "          bins = torch.linspace(start, end, num_bins)\n",
        "          bin_diff = (end - start) / num_bins\n",
        "        elif isinstance(binargs, torch.Tensor):\n",
        "          bins = binargs\n",
        "          bin_diff = self.bins[1] - self.bins[0] if len(self.bins) > 1 else 0.1\n",
        "        else:\n",
        "          raise TypeError(f\"binargs has the wrong type {type(bins)}\")\n",
        "        return bins.to(self.device), torch.tensor([bin_diff]).to(self.device)\n",
        "    def update(self, data):\n",
        "        bin_indices = torch.bucketize(data, self.bins, right=self.right)\n",
        "        indices, counts = bin_indices.unique(return_inverse=False, return_counts = True)\n",
        "        self._counts[indices] += counts\n",
        "    def _labels(self):\n",
        "        def label(i):\n",
        "            start = float(\"-inf\") if i == 0 else self.bins[i-1]\n",
        "            end = float(\"inf\") if i >= len(self.bins) else self.bins[i]\n",
        "            return f\"({start:.4f})-({end:.4f})\"\n",
        "        return [label(i) for i in torch.arange(self.length)]\n",
        "    def labels(self):\n",
        "        def label(i):\n",
        "            start = float(\"-inf\") if i == 0 else self.bins[i-1]\n",
        "            end = float(\"inf\") if i >= len(self.bins) else self.bins[i]\n",
        "            return f\"{start if self.right else end:.4f}\"\n",
        "        return [label(i) for i in torch.arange(self.length)]\n",
        "    def xticks(self, start=True):\n",
        "        res = None\n",
        "        if start:\n",
        "            _start = self.bins[0] - self._bin_diff\n",
        "            res = torch.concat([_start, self.bins])\n",
        "        else:\n",
        "            end = self.bins[-1] + self._bin_diff\n",
        "            res = torch.concat([self.bins, end])\n",
        "        return res.cpu()\n",
        "    def counts(self):\n",
        "        return self._counts.cpu()\n",
        "    def bin_diff(self):\n",
        "        return self._bin_diff.item()\n",
        "OPTIONS = {\n",
        "      \"a\": (\"a\", \"all\"),\n",
        "      \"t\": (\"t\", \"train\", \"trainable\"),\n",
        "      \"nt\": (\"nt\", \"nontrain\", \"nontrainable\"),\n",
        "      \"n\": (\"n\", \"none\", None),\n",
        "  }\n",
        "def map_named_params(callback, model):\n",
        "  for name, param in model.named_parameters():\n",
        "    callback(name, param)\n",
        "def map_named_modules(callback, model):\n",
        "  for name, param in model.named_modules():\n",
        "    callback(name, param)\n",
        "def print_model_compare_any(model, compare, allOrAny, threshold = 1, grads = False):\n",
        "  def compfuncs(attr):\n",
        "    return {\n",
        "      \"gt\": lambda x: getattr(x, attr) > threshold,\n",
        "      \"lt\": lambda x: getattr(x, attr) < threshold,\n",
        "      \"eq\": lambda x: getattr(x, attr) == threshold,\n",
        "      \"neq\": lambda x: getattr(x, attr) != threshold,\n",
        "      \"geq\": lambda x: getattr(x, attr) >= threshold,\n",
        "      \"leq\": lambda x: getattr(x, attr) <= threshold,\n",
        "  }\n",
        "  _attr = \"grad\" if grads else \"data\"\n",
        "  def print_if_satisfies(name, param):\n",
        "    if exists(param, _attr) and getattr(compfuncs(_attr)[compare](param), allOrAny)():\n",
        "      logger.debug(name)\n",
        "  map_named_params(print_if_satisfies, model)\n",
        "\n",
        "def print_info_params_and_ret_hist(\n",
        "    model,\n",
        "    size_type_to_print=None,\n",
        "    print_trainable=\"t\",\n",
        "    binargs = (-1, 1, 100),\n",
        "    calc_param_counts = True,\n",
        "    calc_grad_counts = True,\n",
        "  ):\n",
        "  if isinstance(size_type_to_print, Union[str, type(None)]):\n",
        "    size_type_to_print_all = size_type_to_print\n",
        "    size_type_to_print_trainable = size_type_to_print\n",
        "    size_type_to_print_nontrainable = size_type_to_print\n",
        "  elif isinstance(size_type_to_print, Union[tuple, list]):\n",
        "    if len(size_type_to_print) == 2:\n",
        "      size_type_to_print_all, size_type_to_print_trainable = size_type_to_print\n",
        "      size_type_to_print_nontrainable = size_type_to_print_all\n",
        "    elif len(size_type_to_print) == 3:\n",
        "      size_type_to_print_all, size_type_to_print_trainable, size_type_to_print_nontrainable = size_type_to_print\n",
        "    else:\n",
        "      raise ValueError(f\"size_type_to_print is tuple but has wrong length: {len(size_type_to_print)}\")\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown size_type_to_print: {size_type_to_print}\")\n",
        "\n",
        "  all_params = 0\n",
        "  all_size = 0\n",
        "  trainable_params = 0\n",
        "  trainable_size = 0\n",
        "  nontrainable_params = 0\n",
        "  nontrainable_size = 0\n",
        "  grad_calculable = False\n",
        "  param_counter, grad_counter = None, None\n",
        "  is_trainable_set = print_trainable.lower() in (OPTIONS[\"a\"] + OPTIONS[\"t\"])\n",
        "  is_nontrainable_set = print_trainable.lower() in (OPTIONS[\"a\"] + OPTIONS[\"nt\"])\n",
        "  def update_counts_sizes_then_print(name, param):\n",
        "    nonlocal all_params, all_size, trainable_params, trainable_size, nontrainable_params, nontrainable_size, grad_calculable, param_counter, grad_counter\n",
        "    param_info = get_info_from_param(param)\n",
        "    isTrainable = (param.requires_grad and is_trainable_set)\n",
        "    isNontrainable = (not param.requires_grad and is_nontrainable_set)\n",
        "    isAll = isTrainable and isNontrainable\n",
        "\n",
        "    size_type_to_print_param = None\n",
        "    if isAll:\n",
        "      size_type_to_print_param = size_type_to_print_all\n",
        "    elif isTrainable:\n",
        "      size_type_to_print_param = size_type_to_print_trainable\n",
        "    elif isNontrainable:\n",
        "      size_type_to_print_param = size_type_to_print_nontrainable\n",
        "    all_params += param_info[\"numel\"]\n",
        "    all_size += param_info[\"size\"]\n",
        "    if isTrainable:\n",
        "      trainable_params += param_info[\"numel\"]\n",
        "      trainable_size += param_info[\"size\"]\n",
        "    if isNontrainable:\n",
        "      nontrainable_params += param_info[\"numel\"]\n",
        "      nontrainable_size += param_info[\"size\"]\n",
        "    if (isTrainable or isNontrainable) and binargs != None:\n",
        "      logger.debug(info_str_param(name, param_info, size_type_to_print_param))\n",
        "      if calc_param_counts:\n",
        "        if param_counter == None:\n",
        "          param_counter = BinnedCounter(binargs, device=model.device)\n",
        "        param_counter.update(param.view(-1))\n",
        "      if calc_grad_counts and exists(param, \"grad\"):\n",
        "        if grad_counter == None:\n",
        "          grad_counter = BinnedCounter(binargs, device=model.device)\n",
        "        grad_counter.update(param.grad.view(-1))\n",
        "\n",
        "  map_named_params(update_counts_sizes_then_print, model)\n",
        "\n",
        "  all_size = get_readable_size(all_size, size_type_to_print_all)\n",
        "  trainable_size = get_readable_size(trainable_size, size_type_to_print_trainable)\n",
        "  nontrainable_size = get_readable_size(nontrainable_size, size_type_to_print_nontrainable)\n",
        "\n",
        "  print_str_list = []\n",
        "  print_str_list.append(f\"all params: {all_params}\")\n",
        "  print_str_list.append(f\"all size: {all_size}\")\n",
        "  if is_trainable_set:\n",
        "      print_str_list.append(f\"trainable params: {trainable_params}\")\n",
        "      print_str_list.append(f\"trainable size: {trainable_size}\")\n",
        "      print_str_list.append(f\"trainable%: {100 * trainable_params / all_params}\")\n",
        "  if is_nontrainable_set:\n",
        "      print_str_list.append(f\"nontrainable params: {nontrainable_params}\")\n",
        "      print_str_list.append(f\"nontrainable size: {nontrainable_size}\")\n",
        "      print_str_list.append(f\"nontrainable%: {100 * nontrainable_params / all_params}\")\n",
        "  logger.info(\" || \".join(print_str_list))\n",
        "  return param_counter, grad_counter\n",
        "\n",
        "def plot_hist_params(\n",
        "    param_counter,\n",
        "    grad_counter,\n",
        "    binargs = (-1, 1, 100),\n",
        "    calc_param_counts = True,\n",
        "    calc_grad_counts = True,\n",
        "  ):\n",
        "  def plot_hist_params_ax(ax, counter, title):\n",
        "    logger.set_handle(ax)\n",
        "    xticks = counter.xticks()\n",
        "    yticks = counter.counts()\n",
        "    logger.set_title(title)\n",
        "    logger.set_xticks(xticks, counter.labels(), rotation=45)\n",
        "    logger.set_xlabel(f\"Values {'[)' if counter.right else '(]'}\")\n",
        "    logger.set_ylabel(\"Number of elements\")\n",
        "    logger.bar(xticks, yticks, width=counter.bin_diff())\n",
        "\n",
        "  if (calc_param_counts or calc_grad_counts) and binargs != None:\n",
        "    fig, (ax1,ax2) = plt.subplots(2,1)\n",
        "    if calc_param_counts:\n",
        "      plot_hist_params_ax(ax1, param_counter, \"Parameters\")\n",
        "    if calc_grad_counts and grad_counter != None:\n",
        "      plot_hist_params_ax(ax2, grad_counter, \"Gradients\")\n",
        "    logger.set_handle(plt, True)\n",
        "    logger.gcf().set_size_inches(40,10)\n",
        "    logger.show()\n",
        "    logger.clf()\n",
        "def print_info_params(\n",
        "    model,\n",
        "    size_type_to_print=None,\n",
        "    print_trainable=\"t\",\n",
        "    binargs = (-1, 1, 100),\n",
        "    calc_param_counts = True,\n",
        "    calc_grad_counts = True,\n",
        "):\n",
        "  param_counter, grad_counter = print_info_params_and_ret_hist(\n",
        "    model,\n",
        "    size_type_to_print,\n",
        "    print_trainable,\n",
        "    binargs,\n",
        "  )\n",
        "  plot_hist_params(\n",
        "    param_counter,\n",
        "    grad_counter,\n",
        "    binargs,\n",
        "    calc_param_counts,\n",
        "    calc_grad_counts,\n",
        "  )\n",
        "def compare_models(model1, model2):\n",
        "  allEqual = True\n",
        "  for (name1, param1), (name2, param2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
        "    isEqual = torch.equal(param1, param2)\n",
        "    if not isEqual:\n",
        "      logger.debug(f\"Weights differ: {name1}\")\n",
        "    allEqual = allEqual and isEqual\n",
        "  if allEqual:\n",
        "    logger.info(\"All params equal\")\n",
        "  else:\n",
        "    logger.info(\"Some weights differ\")\n",
        "  return allEqual\n",
        "\n",
        "def print_model(model):\n",
        "  logger.info(\"Model architecture\")\n",
        "  logger.info(model.model)\n",
        "def find_all_linear_names(model):\n",
        "  cls = torch.nn.Linear\n",
        "  lora_module_names = set()\n",
        "  for name, module in model.named_modules():\n",
        "      if isinstance(module, cls):\n",
        "          names = name.split('.')\n",
        "          lora_module_names.add(names[-1])\n",
        "\n",
        "  if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "      lora_module_names.remove('lm_head')\n",
        "  return list(lora_module_names)\n",
        "# LOSS DEBUG\n",
        "def error_report(x, y):\n",
        "  mae = F.l1_loss(x, y)\n",
        "  mse = F.mse_loss(x, y)\n",
        "  logger.info(\n",
        "      f\"Mean absolute error: {mae:>8.5f}\\n\"\n",
        "      f\"Mean squared error:  {mse:>8.5f}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gc.set_debug(gc.DEBUG_LEAK)\n",
        "logger = Logger(logging.DEBUG)\n",
        "# logging.CRITICAL or logging.FATAL: only report the most critical errors.\n",
        "# logging.ERROR: only report errors.\n",
        "# logging.WARNING or logging.WARN: only reports error and warnings. This is the default level used by the library.\n",
        "# logging.INFO: reports error, warnings and basic information.\n",
        "# logging.DEBUG: report all information."
      ],
      "metadata": {
        "id": "nsQrAiEMD3Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Processing"
      ],
      "metadata": {
        "id": "9zzKoVj9DtE9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5zFO00le_as"
      },
      "outputs": [],
      "source": [
        "#from trl.extras.dataset_formatting import get_formatting_func_from_dataset\n",
        "from datasets import DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "import string\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "#MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "DATA_LENGTHS = 8\n",
        "\n",
        "class Processor:\n",
        "  def __init__(self, instruction, model_name, device = {\"\": 0}):\n",
        "    # 'device' is a dictionary that maps devices to model parts. In this case, it is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\n",
        "    self.instruction = Processor.make_one_line(instruction)\n",
        "    self.model_name = model_name\n",
        "    self.label_key = \"labels\"\n",
        "    self.device = device\n",
        "  @staticmethod\n",
        "  def make_one_line(word):\n",
        "    return \" \".join(word.split(\"\\n\")).strip(\" ,\")\n",
        "  def words_to_sentence(self, words: list[str]) -> str:\n",
        "    if not words:\n",
        "        return \"\"\n",
        "    # Handle spacing logic based on punctuation\n",
        "    sentence = \"\"\n",
        "    for i, word in enumerate(words):\n",
        "        # Avoid spaces before punctuation\n",
        "        if i > 0 and sentence and word not in string.punctuation:\n",
        "            sentence += \" \"\n",
        "        sentence += word\n",
        "    # Capitalize first letter if it's a lowercase word\n",
        "    sentence = sentence.capitalize()\n",
        "    return sentence\n",
        "  def sample_n(self, dataset, n, shuffle=True, seed=None):\n",
        "    ns = None\n",
        "    if isinstance(n, Iterable):\n",
        "      ns = n\n",
        "    elif isinstance(n, int):\n",
        "      ns = [n]*len(dataset.keys())\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown n: {n}\")\n",
        "    def select_n(dataset, n):\n",
        "      return dataset.select(range(n))\n",
        "    def select_split_n_shuffled(dataset, n, shuffle=True):\n",
        "      return select_n(dataset.shuffle(seed=seed), n) if shuffle else select_n(dataset, n)\n",
        "    return DatasetDict(\n",
        "        {\n",
        "            split_name: select_split_n_shuffled(split, ns[i], shuffle) for i, (split_name, split) in enumerate(dataset.items())\n",
        "        }\n",
        "    )\n",
        "  def split(self, dataset):\n",
        "    train_dataset = dataset[\"train\"]\n",
        "    val_dataset = dataset[\"validation\"]\n",
        "    test_dataset = dataset[\"test\"]\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "  def get_tokenizer(self):\n",
        "    if exists(self, \"tokenizer\"):\n",
        "      return self.tokenizer\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
        "    #self.tokenizer.pad_token = self.tokenizer.unk_token\n",
        "    #self.tokenizer.pad_token_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)\n",
        "    self.tokenizer.padding_side = 'left' # For decoder-only models\n",
        "    return self.tokenizer\n",
        "  def get_model(self):\n",
        "    if exists(self, \"model\"):\n",
        "      return self.model\n",
        "    if torch.cuda.is_bf16_supported():\n",
        "      compute_dtype = torch.bfloat16 # BrainFloat 16 bits, can represent 32 bit with less precision (Mixed precision)\n",
        "    else:\n",
        "      compute_dtype = torch.float16 # 16 bit\n",
        "\n",
        "    max_seq_length = 4096\n",
        "\n",
        "    bnb_config = None\n",
        "    if self.device != \"cpu\":\n",
        "      bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, # num of bits when quantizing\n",
        "        bnb_4bit_quant_type=\"nf4\",# Quantization type 4 bits NormalFloat\n",
        "        bnb_4bit_use_double_quant=True,# Use nested quantization\n",
        "        bnb_4bit_compute_dtype=compute_dtype, # num of bits when computing\n",
        "        max_seq_length=max_seq_length,\n",
        "      )\n",
        "    attn_implementation = 'eager' # Flash attention not supported on gpus older than ampere\n",
        "\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "      self.model_name,\n",
        "      torch_dtype=compute_dtype,\n",
        "      trust_remote_code=True,\n",
        "      device_map=self.device,\n",
        "      attn_implementation=attn_implementation,\n",
        "      quantization_config=bnb_config,\n",
        "    )\n",
        "    return self.model\n",
        "  def create_sentence_triples_columns(self, row_raw):\n",
        "    row = eval(row_raw[\"text\"])\n",
        "    sentence = self.words_to_sentence(row[\"token\"])\n",
        "    triples = str([(row[\"h\"][\"name\"], row[\"relation\"], row[\"t\"][\"name\"])])\n",
        "    return {\"sentence\": sentence, self.label_key: triples}\n",
        "  def create_message_column(self, row, training_mode=True):\n",
        "    messages = []\n",
        "    user = {\n",
        "        \"content\": f\"{self.instruction}\\n Sentence: {row['sentence']}\",\n",
        "        \"role\": \"user\"\n",
        "    }\n",
        "    messages.append(user)\n",
        "    if training_mode:\n",
        "      assistant = {\n",
        "          \"content\": row[self.label_key],\n",
        "          \"role\": \"assistant\"\n",
        "      }\n",
        "      messages.append(assistant)\n",
        "    return {\"messages\": messages}\n",
        "  def format_dataset_chatml_and_tokenize(self, row, training_mode=True):\n",
        "    return self.tokenizer.apply_chat_template(\n",
        "      row[\"messages\"],\n",
        "      add_generation_prompt=not training_mode,\n",
        "      tokenize=True,\n",
        "      return_dict=True,\n",
        "      add_special_tokens=False,\n",
        "      truncation=True,\n",
        "      padding=False,\n",
        "      max_length=512,\n",
        "      return_overflowing_tokens=False,\n",
        "      return_length=False,\n",
        "    )\n",
        "  def format_dataset_chatml(self, row, training_mode=True):\n",
        "    return {\"messages\": self.tokenizer.apply_chat_template(\n",
        "      row[\"messages\"],\n",
        "      add_generation_prompt=not training_mode,\n",
        "      tokenize=False,\n",
        "      add_special_tokens=False,\n",
        "      truncation=True,\n",
        "      padding=False,\n",
        "      max_length=512,\n",
        "      return_overflowing_tokens=False,\n",
        "      return_length=False,\n",
        "    )}\n",
        "  def process(self, dataset):\n",
        "    logger.info(\"Creating sentence-triples columns\")\n",
        "    dataset = dataset.map(self.create_sentence_triples_columns, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "    tokenizer = self.get_tokenizer()\n",
        "\n",
        "    rndindex = random.randrange(DATA_LENGTHS)\n",
        "\n",
        "    def get_remove_columns():\n",
        "      remove_columns = list(dataset[data_split].column_names)\n",
        "      if not training_mode:\n",
        "        remove_columns.remove(self.label_key)\n",
        "      return remove_columns\n",
        "\n",
        "    for data_split in dataset.keys():\n",
        "      logger.info(f\"Split {data_split}\")\n",
        "      logger.debug(dataset[data_split][rndindex])\n",
        "      training_mode = data_split==\"train\"\n",
        "      remove_columns = get_remove_columns()\n",
        "      logger.info(f\"Mapping columns to 'messages'\")\n",
        "      dataset[data_split] = dataset[data_split].map(\n",
        "        self.create_message_column,\n",
        "        remove_columns=remove_columns,\n",
        "        fn_kwargs={\"training_mode\": training_mode}\n",
        "      )\n",
        "      logger.debug(dataset[data_split][rndindex])\n",
        "      remove_columns = get_remove_columns()\n",
        "      logger.info(\"Formatting to chatml format\")\n",
        "      dataset[data_split] = dataset[data_split].map(\n",
        "        self.format_dataset_chatml_and_tokenize,\n",
        "        remove_columns=remove_columns,\n",
        "        fn_kwargs={\"training_mode\": training_mode},\n",
        "      )\n",
        "      logger.debug(dataset[data_split][rndindex])\n",
        "    return self.split(dataset)\n",
        "INSTRUCTION_WITH_PREDICATES = \"\"\"\n",
        "List of predicates is ['org:founded', 'org:subsidiaries', 'per:date_of_birth', 'per:cause_of_death',\n",
        "'per:age', 'per:stateorprovince_of_birth', 'per:countries_of_residence', 'per:country_of_birth',\n",
        "'per:stateorprovinces_of_residence', 'org:website', 'per:cities_of_residence', 'per:parents',\n",
        "'per:employee_of', 'NA', 'per:city_of_birth', 'org:parents', 'org:political/religious_affiliation',\n",
        "'per:schools_attended', 'per:country_of_death', 'per:children', 'org:top_members/employees',\n",
        "'per:date_of_death', 'org:members', 'org:alternate_names', 'per:religion', 'org:member_of',\n",
        "'org:city_of_headquarters', 'per:origin', 'org:shareholders', 'per:charges', 'per:title',\n",
        "'org:number_of_employees/members', 'org:dissolved', 'org:country_of_headquarters', 'per:alternate_names',\n",
        "'per:siblings', 'org:stateorprovince_of_headquarters', 'per:spouse', 'per:other_family', 'per:city_of_death',\n",
        "'per:stateorprovince_of_death', 'org:founded_by'].\n",
        "What Subject-Predicate-Object triples are included in the following sentence?\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Processor(INSTRUCTION_WITH_PREDICATES, MODEL_NAME)\n",
        "model = processor.get_model()\n",
        "tokenizer = processor.get_tokenizer()"
      ],
      "metadata": {
        "id": "sbCgkvlZDeLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_info_params(\n",
        "    model,\n",
        "    size_type_to_print=(\"GB\", \"MB\"),\n",
        "    print_trainable=\"a\",\n",
        "    calc_param_counts = False,\n",
        "    calc_grad_counts = False,\n",
        ")"
      ],
      "metadata": {
        "id": "1cFRQbBn-0Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRkeo4zLJ3-"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEBUG Cell 1"
      ],
      "metadata": {
        "id": "Zpa7JJRgDE6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.generation.utils import (\n",
        "    GenerationMode,\n",
        "    GenerationConfig,\n",
        "    StoppingCriteriaList,\n",
        "    LogitsProcessorList,\n",
        "    GenerateBeamDecoderOnlyOutput,\n",
        "    GenerateBeamEncoderDecoderOutput,\n",
        "    stack_model_outputs,\n",
        "    _split_model_inputs,\n",
        ")\n",
        "from transformers.generation.beam_search import BeamScorer, BeamHypotheses\n",
        "import inspect\n",
        "from torch import nn\n",
        "from collections import UserDict\n",
        "from transformers.generation.logits_process import LogitsProcessor\n",
        "from collections import Counter\n",
        "\n",
        "class CounterPrint:\n",
        "  def __init__(self, enable=True):\n",
        "    self.counter = Counter()\n",
        "    self.enable = enable\n",
        "  def __call__(self, label, *args, **kwargs):\n",
        "    self.counter[label] += 1\n",
        "    if self.enable:\n",
        "      logger.info(self.counter[label], label, *args,**kwargs)\n",
        "\n",
        "class TemperatureLogitsWarper(LogitsProcessor):\n",
        "    def __init__(self, temperature: float):\n",
        "        if not isinstance(temperature, float) or not (temperature > 0):\n",
        "            except_msg = (\n",
        "                f\"`temperature` (={temperature}) has to be a strictly positive float, otherwise your next token \"\n",
        "                \"scores will be invalid.\"\n",
        "            )\n",
        "            if isinstance(temperature, float) and temperature == 0.0:\n",
        "                except_msg += \" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\n",
        "            raise ValueError(except_msg)\n",
        "\n",
        "        self.temperature = temperature\n",
        "        self.counter = 0\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        scores_processed = scores / self.temperature\n",
        "        if self.counter == 65:\n",
        "          torch.set_printoptions(edgeitems=20000)\n",
        "          print(\"scores\")\n",
        "          print(scores)\n",
        "          print(scores_processed)\n",
        "        self.counter += 1\n",
        "        return scores_processed\n",
        "\n",
        "class BeamSearchScorer(BeamScorer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "        num_beams: int,\n",
        "        device: torch.device,\n",
        "        length_penalty: Optional[float] = 1.0,\n",
        "        do_early_stopping: Optional[Union[bool, str]] = False,\n",
        "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
        "        num_beam_groups: Optional[int] = 1,\n",
        "        max_length: Optional[int] = None,\n",
        "    ):\n",
        "        self.num_beams = num_beams\n",
        "        self.device = device\n",
        "        self.length_penalty = length_penalty\n",
        "        self.do_early_stopping = do_early_stopping\n",
        "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
        "        self.num_beam_groups = num_beam_groups\n",
        "        self.group_size = self.num_beams // self.num_beam_groups\n",
        "\n",
        "        self._is_init = False\n",
        "        # self._beam_hyps[i*self.num_beam_groups+j] is the beam_hyps of the j-th group in the i-th mini-batch.\n",
        "        # If group_beam_search is not used, the list consists of `batch_size` beam_hyps.\n",
        "        self._beam_hyps = [\n",
        "            BeamHypotheses(\n",
        "                num_beams=self.group_size,\n",
        "                length_penalty=self.length_penalty,\n",
        "                early_stopping=self.do_early_stopping,\n",
        "                max_length=max_length,\n",
        "            )\n",
        "            for _ in range(batch_size * self.num_beam_groups)\n",
        "        ]\n",
        "        # self._done[i*self.num_beam_groups+j] indicates whether the generation of the beam_hyps of the j-th group\n",
        "        # in the i-th mini-batch is complete.\n",
        "        self._done = torch.tensor(\n",
        "            [False for _ in range(batch_size * self.num_beam_groups)], dtype=torch.bool, device=self.device\n",
        "        )\n",
        "        #print(f\"(init) self._done: {self._done}, self._beam_hyps: {self._beam_hyps}\")\n",
        "        if not isinstance(num_beams, int) or num_beams <= 1:\n",
        "            raise ValueError(\n",
        "                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n",
        "                \" one should make use of `greedy_search` instead.\"\n",
        "            )\n",
        "\n",
        "        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n",
        "            raise ValueError(\n",
        "                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n",
        "                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def is_done(self) -> bool:\n",
        "        return self._done.all()\n",
        "\n",
        "    def process(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        next_scores: torch.FloatTensor,\n",
        "        next_tokens: torch.LongTensor,\n",
        "        next_indices: torch.LongTensor,\n",
        "        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n",
        "        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n",
        "        beam_indices: Optional[torch.LongTensor] = None,\n",
        "        group_index: Optional[int] = 0,\n",
        "        decoder_prompt_len: Optional[int] = 0,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        # add up to the length which the next_scores is calculated on (including decoder prompt)\n",
        "        cur_len = input_ids.shape[-1] + 1\n",
        "        batch_size = len(self._beam_hyps) // self.num_beam_groups\n",
        "\n",
        "        if not (batch_size == (input_ids.shape[0] // self.group_size)):\n",
        "            if self.num_beam_groups > 1:\n",
        "                raise ValueError(\n",
        "                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n",
        "                    f\"size of {self.group_size} is expected by the beam scorer.\"\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n",
        "                    f\"{self.group_size} is expected by the beam scorer.\"\n",
        "                )\n",
        "\n",
        "        device = input_ids.device\n",
        "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
        "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
        "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
        "\n",
        "        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n",
        "            if isinstance(eos_token_id, int):\n",
        "                eos_token_id = [eos_token_id]\n",
        "            eos_token_id = torch.tensor(eos_token_id)\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            batch_group_idx = batch_idx * self.num_beam_groups + group_index\n",
        "            #print(batch_idx, batch_group_idx, batch_size)\n",
        "            #print(f\"(long before) input_ids.device: {input_ids.device}\")\n",
        "            #print(f\"(long before) self._done.device: {self._done.device}\")\n",
        "            #print(f\"(long before) self._done.shape: {self._done.shape}\")\n",
        "            #print(f\"(long before) self._beam_hyps: {self._beam_hyps}\")\n",
        "            #print(self._done)\n",
        "            if self._done[batch_group_idx]:\n",
        "                if self.num_beams < len(self._beam_hyps[batch_group_idx]):\n",
        "                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n",
        "                if eos_token_id is None or pad_token_id is None:\n",
        "                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n",
        "                # pad the batch\n",
        "                next_beam_scores[batch_idx, :] = 0\n",
        "                next_beam_tokens[batch_idx, :] = pad_token_id\n",
        "                next_beam_indices[batch_idx, :] = 0\n",
        "                continue\n",
        "\n",
        "            # next tokens for this sentence\n",
        "            beam_idx = 0\n",
        "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
        "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
        "            ):\n",
        "                batch_beam_idx = batch_idx * self.group_size + next_index\n",
        "                # add to generated hypotheses if end of sentence\n",
        "                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n",
        "                    # if beam_token does not belong to top num_beams tokens, it should not be added\n",
        "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
        "                    if is_beam_token_worse_than_top_num_beams:\n",
        "                        continue\n",
        "                    if beam_indices is not None:\n",
        "                        beam_index = beam_indices[batch_beam_idx]\n",
        "                        beam_index = beam_index + (batch_beam_idx,)\n",
        "                    else:\n",
        "                        beam_index = None\n",
        "\n",
        "                    self._beam_hyps[batch_group_idx].add(\n",
        "                        input_ids[batch_beam_idx].clone(),\n",
        "                        next_score.item(),\n",
        "                        beam_indices=beam_index,\n",
        "                        generated_len=cur_len - decoder_prompt_len,\n",
        "                    )\n",
        "                else:\n",
        "                    # add next predicted token since it is not eos_token\n",
        "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
        "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
        "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
        "                    beam_idx += 1\n",
        "\n",
        "                # once the beam for next step is full, don't add more tokens to it.\n",
        "                if beam_idx == self.group_size:\n",
        "                    break\n",
        "\n",
        "            if beam_idx < self.group_size:\n",
        "                raise ValueError(\n",
        "                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n",
        "                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n",
        "                )\n",
        "\n",
        "            # Check if we are done so that we can save a pad step if all(done)\n",
        "            #print(f\"(before) self._done: {self._done}, self._beam_hyps: {self._beam_hyps}\")\n",
        "            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(\n",
        "                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n",
        "            )\n",
        "            #print(f\"(after) self._done: {self._done}, self._beam_hyps: {self._beam_hyps}\")\n",
        "\n",
        "        return UserDict(\n",
        "            {\n",
        "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
        "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
        "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def finalize(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        final_beam_scores: torch.FloatTensor,\n",
        "        final_beam_tokens: torch.LongTensor,\n",
        "        final_beam_indices: torch.LongTensor,\n",
        "        max_length: int,\n",
        "        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n",
        "        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n",
        "        beam_indices: Optional[torch.LongTensor] = None,\n",
        "        decoder_prompt_len: Optional[int] = 0,\n",
        "    ):\n",
        "        batch_size = len(self._beam_hyps) // self.num_beam_groups\n",
        "\n",
        "        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n",
        "            if isinstance(eos_token_id, int):\n",
        "                eos_token_id = [eos_token_id]\n",
        "            eos_token_id = torch.tensor(eos_token_id)\n",
        "\n",
        "        # finalize all open beam hypotheses and add to generated hypotheses\n",
        "        for batch_group_idx, beam_hyp in enumerate(self._beam_hyps):\n",
        "            if self._done[batch_group_idx]:\n",
        "                continue\n",
        "\n",
        "            # all open beam hypotheses are added to the beam hypothesis\n",
        "            # beam hypothesis class automatically keeps the best beams\n",
        "            for index_per_group in range(self.group_size):\n",
        "                batch_beam_idx = batch_group_idx * self.group_size + index_per_group\n",
        "                final_score = final_beam_scores[batch_beam_idx].item()\n",
        "                final_tokens = input_ids[batch_beam_idx]\n",
        "                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n",
        "                generated_len = final_tokens.shape[-1] - decoder_prompt_len\n",
        "                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n",
        "\n",
        "        # select the best hypotheses\n",
        "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
        "        best = []\n",
        "        best_indices = []\n",
        "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
        "\n",
        "        # retrieve best hypotheses\n",
        "        for i in range(batch_size):\n",
        "            beam_hyps_in_batch = self._beam_hyps[i * self.num_beam_groups : (i + 1) * self.num_beam_groups]\n",
        "            candidate_beams = [beam for beam_hyp in beam_hyps_in_batch for beam in beam_hyp.beams]\n",
        "            sorted_hyps = sorted(candidate_beams, key=lambda x: x[0])\n",
        "            for j in range(self.num_beam_hyps_to_keep):\n",
        "                best_hyp_tuple = sorted_hyps.pop()\n",
        "                best_score = best_hyp_tuple[0]\n",
        "                best_hyp = best_hyp_tuple[1]\n",
        "                best_index = best_hyp_tuple[2]\n",
        "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
        "\n",
        "                # append hyp to lists\n",
        "                best.append(best_hyp)\n",
        "\n",
        "                # append indices to list\n",
        "                best_indices.append(best_index)\n",
        "\n",
        "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
        "\n",
        "        # prepare for adding eos\n",
        "        sent_lengths_max = sent_lengths.max().item() + 1\n",
        "        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n",
        "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
        "\n",
        "        if len(best_indices) > 0 and best_indices[0] is not None:\n",
        "            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
        "        else:\n",
        "            indices = None\n",
        "\n",
        "        # shorter batches are padded if needed\n",
        "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
        "            if pad_token_id is None:\n",
        "                raise ValueError(\"`pad_token_id` has to be defined\")\n",
        "            decoded.fill_(pad_token_id)\n",
        "\n",
        "        if indices is not None:\n",
        "            indices.fill_(-1)\n",
        "\n",
        "        # fill with hypotheses and eos_token_id if the latter fits in\n",
        "        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n",
        "            decoded[i, : sent_lengths[i]] = hypo\n",
        "\n",
        "            if indices is not None:\n",
        "                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n",
        "\n",
        "            if sent_lengths[i] < sent_max_len:\n",
        "                # inserting only the first eos_token_id\n",
        "                decoded[i, sent_lengths[i]] = eos_token_id[0]\n",
        "\n",
        "        return UserDict(\n",
        "            {\n",
        "                \"sequences\": decoded,\n",
        "                \"sequence_scores\": best_scores,\n",
        "                \"beam_indices\": indices,\n",
        "            }\n",
        "        )\n",
        "\n",
        "def _beam_search(\n",
        "    self,\n",
        "    input_ids: torch.LongTensor,\n",
        "    beam_scorer: BeamScorer,\n",
        "    logits_processor: LogitsProcessorList,\n",
        "    stopping_criteria: StoppingCriteriaList,\n",
        "    generation_config: GenerationConfig,\n",
        "    synced_gpus: bool,\n",
        "    **model_kwargs,\n",
        "):\n",
        "        print(\"generation_config:\")\n",
        "        print(generation_config.to_dict())\n",
        "        print(\"logits_processor:\")\n",
        "        print(logits_processor)\n",
        "        # init values\n",
        "        pad_token_id = generation_config._pad_token_tensor\n",
        "        eos_token_id = generation_config._eos_token_tensor\n",
        "        output_attentions = generation_config.output_attentions\n",
        "        output_hidden_states = generation_config.output_hidden_states\n",
        "        output_scores = generation_config.output_scores\n",
        "        output_logits = generation_config.output_logits\n",
        "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
        "        sequential = generation_config.low_memory\n",
        "        do_sample = generation_config.do_sample\n",
        "\n",
        "        batch_size = len(beam_scorer._beam_hyps)\n",
        "        num_beams = beam_scorer.num_beams\n",
        "\n",
        "        batch_beam_size, cur_len = input_ids.shape\n",
        "        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
        "\n",
        "        if num_beams * batch_size != batch_beam_size:\n",
        "            raise ValueError(\n",
        "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
        "            )\n",
        "\n",
        "        # init attention / hidden states / scores tuples\n",
        "        scores = () if (return_dict_in_generate and output_scores) else None\n",
        "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
        "        beam_indices = (\n",
        "            tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n",
        "        )\n",
        "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
        "\n",
        "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
        "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
        "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
        "            encoder_hidden_states = (\n",
        "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
        "            )\n",
        "\n",
        "        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n",
        "        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n",
        "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
        "\n",
        "        this_peer_finished = False\n",
        "\n",
        "        decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n",
        "        _tmp_counter=0\n",
        "        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
        "            counter_print = CounterPrint()\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            print(f\"input_ids: {input_ids.shape}, model_inputs: {type_shape(model_inputs)}, model_kwargs: {type_shape(model_kwargs)}\")\n",
        "\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#1\n",
        "\n",
        "            # prepare variable output controls (note: some models won't accept all output controls)\n",
        "            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
        "            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
        "\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#2\n",
        "\n",
        "            # if sequential is True, split the input to batches of batch_size and run sequentially\n",
        "            if sequential:\n",
        "                if any(\n",
        "                    model_name in self.__class__.__name__.lower()\n",
        "                    for model_name in [\n",
        "                        \"fsmt\",\n",
        "                        \"reformer\",\n",
        "                        \"ctrl\",\n",
        "                        \"gpt_bigcode\",\n",
        "                        \"transo_xl\",\n",
        "                        \"xlnet\",\n",
        "                        \"cpm\",\n",
        "                        \"jamba\",\n",
        "                    ]\n",
        "                ):\n",
        "                    raise RuntimeError(\n",
        "                        f\"Currently generation for {self.__class__.__name__} is not supported \"\n",
        "                        f\"for `low_memory beam_search`. Please open an issue on GitHub if you need this feature.\"\n",
        "                    )\n",
        "                counter_print(\"if sequential\")\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#3\n",
        "                inputs_per_sub_batches = _split_model_inputs(\n",
        "                    model_inputs,\n",
        "                    split_size=batch_size,\n",
        "                    full_batch_size=batch_beam_size,\n",
        "                    config=self.config.get_text_config(),\n",
        "                )\n",
        "                outputs_per_sub_batch = [\n",
        "                    self(**inputs_per_sub_batch, return_dict=True) for inputs_per_sub_batch in inputs_per_sub_batches\n",
        "                ]\n",
        "\n",
        "                outputs = stack_model_outputs(outputs_per_sub_batch, self.config.get_text_config())\n",
        "            else:  # Unchanged original behavior\n",
        "                counter_print(\"else any\")\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#3\n",
        "                outputs = self(**model_inputs, return_dict=True)\n",
        "\n",
        "            counter_print(\"outputs\", type_shape(outputs))#1\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#4\n",
        "\n",
        "            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs,\n",
        "                model_kwargs,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            )\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                cur_len = cur_len + 1\n",
        "                continue\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#5\n",
        "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n",
        "            # (the clone itself is always small)\n",
        "            # .float() is needed to retain precision for later logits manipulations\n",
        "            next_token_logits = outputs.logits[:, -1, :].clone().float()\n",
        "            next_token_logits = next_token_logits.to(input_ids.device)\n",
        "\n",
        "            counter_print(\"next_token_logits\", next_token_logits)\n",
        "            next_token_scores = nn.functional.log_softmax(\n",
        "                next_token_logits, dim=-1\n",
        "            )  # (batch_size * num_beams, vocab_size)\n",
        "            def noninf(_tensor):\n",
        "              if _tmp_counter == 65:\n",
        "                indices = (_tensor != -torch.inf).nonzero()\n",
        "                #vals = _tensor[indices]\n",
        "                return indices, #vals\n",
        "              return None\n",
        "            def isnan(_tensor):\n",
        "              return _tensor.isnan().any()\n",
        "            def printtofile(filename, var):\n",
        "              torch.set_printoptions(edgeitems=self.vocab_size//2)\n",
        "              with open(filename, \"w\") as f:\n",
        "                print(var, end=\"\", file=f)\n",
        "              torch.set_printoptions(edgeitems=3)\n",
        "            counter_print(f\"_tmp_counter: {_tmp_counter}\")\n",
        "            counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#1\n",
        "            counter_print(\"noninf next_token_scores\", noninf(next_token_scores))#1\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#6\n",
        "            counter_print(\"logits_processor\", logits_processor)#1\n",
        "            counter_print(\"temperature\", generation_config.temperature)#1\n",
        "            counter_print(\"input_ids\", input_ids, input_ids.shape)#1\n",
        "            counter_print(\"noninf input_ids\", noninf(input_ids))#1\n",
        "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
        "            if _tmp_counter == 65:\n",
        "              printtofile(\"next_token_scores\", next_token_scores)\n",
        "              printtofile(\"next_token_scores_processed\", next_token_scores_processed)\n",
        "            counter_print(\"next_token_scores_processed\", next_token_scores_processed, next_token_scores_processed.shape)\n",
        "            counter_print(\"noninf next_token_scores_processed\", noninf(next_token_scores_processed))#2\n",
        "            counter_print(\"beam_scores\", beam_scores, beam_scores.shape)#1\n",
        "            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
        "                next_token_scores_processed\n",
        "            )\n",
        "            counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#2\n",
        "            counter_print(\"noninf next_token_scores\", noninf(next_token_scores))#2\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#7\n",
        "            # Store scores, attentions and hidden_states when required\n",
        "            if return_dict_in_generate:\n",
        "                counter_print(\"if return_dict_in_generate\")\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "                if output_scores:\n",
        "                    scores += (next_token_scores_processed,)\n",
        "                if output_logits:\n",
        "                    raw_logits += (next_token_logits,)\n",
        "                if output_attentions:\n",
        "                    decoder_attentions += (\n",
        "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
        "                    )\n",
        "                    if self.config.is_encoder_decoder:\n",
        "                        cross_attentions += (outputs.cross_attentions,)\n",
        "                if output_hidden_states:\n",
        "                    decoder_hidden_states += (\n",
        "                        (outputs.decoder_hidden_states,)\n",
        "                        if self.config.is_encoder_decoder\n",
        "                        else (outputs.hidden_states,)\n",
        "                    )\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#8\n",
        "            # reshape for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
        "            counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#3\n",
        "            counter_print(\"noninf next_token_scores\", noninf(next_token_scores))#3\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#9\n",
        "            n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n",
        "            n_tokens_to_keep = max(2, 1 + n_eos_tokens) * num_beams\n",
        "            if do_sample:\n",
        "                counter_print(\"if do_sample\")\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#10\n",
        "                probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
        "                counter_print(\"isnan probs\", isnan(probs))#1\n",
        "                counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#4\n",
        "                counter_print(\"noninf next_token_scores\", noninf(next_token_scores))#4\n",
        "                counter_print(\"n_tokens_to_keep\", n_tokens_to_keep)#1\n",
        "                next_tokens = torch.multinomial(probs, num_samples=n_tokens_to_keep)\n",
        "                counter_print(\"next_tokens\", next_tokens, next_tokens.shape)#1\n",
        "                next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
        "                counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#5\n",
        "                next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
        "                counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#6\n",
        "                next_tokens = torch.gather(next_tokens, -1, _indices)\n",
        "                counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#7\n",
        "                counter_print(\"next_tokens\", next_tokens, next_tokens.shape)#2\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#11\n",
        "            else:\n",
        "                counter_print(\"else do_sample\")\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#10\n",
        "                next_token_scores, next_tokens = torch.topk(\n",
        "                    next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n",
        "                )\n",
        "                counter_print(\"next_token_scores\", next_token_scores, next_token_scores.shape)#4\n",
        "                counter_print(\"noninf next_token_scores\", noninf(next_token_scores))#4\n",
        "                counter_print(\"next_tokens\", next_tokens, next_tokens.shape)#1\n",
        "                counter_print(\"beam_scorer._done\", beam_scorer._done)#11\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)#12\n",
        "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "            #print(f\"before process\")\n",
        "            #print(f\"before process beam_scorer._done: {beam_scorer._done}\")\n",
        "            # stateless\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=eos_token_id,\n",
        "                beam_indices=beam_indices,\n",
        "                decoder_prompt_len=decoder_prompt_len,\n",
        "            )\n",
        "            #print(f\"after process\")\n",
        "            #counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "\n",
        "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
        "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
        "            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n",
        "            # (that way the memory peak does not include outputs.logits)\n",
        "            del outputs\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "            if model_kwargs.get(\"past_key_values\", None) is not None:\n",
        "                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n",
        "                    model_kwargs[\"past_key_values\"], beam_idx\n",
        "                )\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "            if return_dict_in_generate and output_scores:\n",
        "                beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n",
        "\n",
        "            # increase cur_len\n",
        "            cur_len = cur_len + 1\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "            if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n",
        "                this_peer_finished = True\n",
        "            counter_print(\"beam_scorer._done\", beam_scorer._done)\n",
        "            _tmp_counter += 1\n",
        "\n",
        "        sequence_outputs = beam_scorer.finalize(\n",
        "            input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            beam_indices=beam_indices,\n",
        "            decoder_prompt_len=decoder_prompt_len,\n",
        "        )\n",
        "\n",
        "        if return_dict_in_generate:\n",
        "            if not output_scores:\n",
        "                sequence_outputs[\"sequence_scores\"] = None\n",
        "\n",
        "            if self.config.is_encoder_decoder:\n",
        "                return GenerateBeamEncoderDecoderOutput(\n",
        "                    sequences=sequence_outputs[\"sequences\"],\n",
        "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
        "                    encoder_attentions=encoder_attentions,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    decoder_attentions=decoder_attentions,\n",
        "                    cross_attentions=cross_attentions,\n",
        "                    decoder_hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "            else:\n",
        "                return GenerateBeamDecoderOnlyOutput(\n",
        "                    sequences=sequence_outputs[\"sequences\"],\n",
        "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
        "                    attentions=decoder_attentions,\n",
        "                    hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "        else:\n",
        "            return sequence_outputs[\"sequences\"]\n",
        "@torch.no_grad()\n",
        "def generate_debug(\n",
        "    self,\n",
        "    inputs = None,\n",
        "    generation_config = None,\n",
        "    logits_processor = None,\n",
        "    stopping_criteria = None,\n",
        "    prefix_allowed_tokens_fn = None,\n",
        "    synced_gpus = None,\n",
        "    assistant_model = None,\n",
        "    streamer = None,\n",
        "    negative_prompt_ids = None,\n",
        "    negative_prompt_attention_mask = None,\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
        "        self._validate_model_class()\n",
        "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
        "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
        "\n",
        "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n",
        "        self._validate_model_kwargs(model_kwargs.copy())\n",
        "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
        "\n",
        "        # 2. Set generation parameters if not already defined\n",
        "        \"\"\"\n",
        "        if synced_gpus is None:\n",
        "            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n",
        "        \"\"\"\n",
        "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
        "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
        "\n",
        "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
        "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
        "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
        "\n",
        "        # 3. Define model inputs\n",
        "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
        "            inputs, generation_config.bos_token_id, model_kwargs\n",
        "        )\n",
        "        batch_size = inputs_tensor.shape[0]\n",
        "\n",
        "        device = inputs_tensor.device\n",
        "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
        "\n",
        "        # decoder-only models must use left-padding for batched generation.\n",
        "        \"\"\"\n",
        "        if not self.config.is_encoder_decoder and not is_torchdynamo_compiling():\n",
        "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
        "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
        "            if (\n",
        "                generation_config._pad_token_tensor is not None\n",
        "                and batch_size > 1\n",
        "                and len(inputs_tensor.shape) == 2\n",
        "                and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0\n",
        "            ):\n",
        "                logger.warning(\n",
        "                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
        "                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
        "                )\n",
        "        \"\"\"\n",
        "        # 4. Define other model kwargs\n",
        "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
        "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
        "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
        "            generation_config.use_cache = True\n",
        "\n",
        "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
        "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
        "                inputs_tensor, generation_config, model_kwargs\n",
        "            )\n",
        "        elif kwargs_has_attention_mask:\n",
        "            # TODO (joao): generalize this check with other types of inputs\n",
        "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
        "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
        "\n",
        "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
        "            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n",
        "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
        "                inputs_tensor, model_kwargs, model_input_name, generation_config\n",
        "            )\n",
        "\n",
        "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
        "        if self.config.is_encoder_decoder:\n",
        "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
        "                batch_size=batch_size,\n",
        "                model_input_name=model_input_name,\n",
        "                model_kwargs=model_kwargs,\n",
        "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
        "                device=inputs_tensor.device,\n",
        "            )\n",
        "        else:\n",
        "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
        "\n",
        "        if generation_config.token_healing:\n",
        "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
        "\n",
        "        if streamer is not None:\n",
        "            streamer.put(input_ids.cpu())\n",
        "\n",
        "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
        "        input_ids_length = input_ids.shape[-1]\n",
        "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
        "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
        "        generation_config = self._prepare_generated_length(\n",
        "            generation_config=generation_config,\n",
        "            has_default_max_length=has_default_max_length,\n",
        "            has_default_min_length=has_default_min_length,\n",
        "            model_input_name=model_input_name,\n",
        "            inputs_tensor=inputs_tensor,\n",
        "            input_ids_length=input_ids_length,\n",
        "        )\n",
        "\n",
        "        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n",
        "        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n",
        "        # dynamically overrides this value as it can need more than the last token logits\n",
        "        #if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
        "        #    model_kwargs[\"logits_to_keep\"] = 1\n",
        "\n",
        "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
        "\n",
        "        # 7. Prepare the cache.\n",
        "        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n",
        "        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n",
        "        # - `max_length`, prepared above, is used to determine the maximum cache length\n",
        "        max_cache_length = generation_config.max_length - 1\n",
        "        if (\n",
        "            inputs_tensor.shape[1] != input_ids_length\n",
        "            and model_input_name == \"inputs_embeds\"\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            max_cache_length += inputs_tensor.shape[1]\n",
        "        self._prepare_cache_for_generation(\n",
        "            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n",
        "        )\n",
        "\n",
        "        # 8. determine generation mode\n",
        "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
        "\n",
        "        if streamer is not None and (generation_config.num_beams > 1):\n",
        "            raise ValueError(\n",
        "                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        if not is_torchdynamo_compiling() and self.device.type != input_ids.device.type:\n",
        "            warnings.warn(\n",
        "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
        "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
        "                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
        "                \" Please make sure that you have put `input_ids` to the\"\n",
        "                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n",
        "                \" running `.generate()`.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "        \"\"\"\n",
        "        # 9. prepare logits processors and stopping criteria\n",
        "        prepared_logits_processor = self._get_logits_processor(\n",
        "            generation_config=generation_config,\n",
        "            input_ids_seq_length=input_ids_length,\n",
        "            encoder_input_ids=inputs_tensor,\n",
        "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "            logits_processor=logits_processor,\n",
        "            device=inputs_tensor.device,\n",
        "            model_kwargs=model_kwargs,\n",
        "            negative_prompt_ids=negative_prompt_ids,\n",
        "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
        "        )\n",
        "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
        "            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n",
        "        )\n",
        "\n",
        "        # Set model_kwargs `use_cache` so we can use it later in forward runs\n",
        "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
        "\n",
        "        if generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n",
        "            # 11. prepare beam search scorer\n",
        "            beam_scorer = BeamSearchScorer(\n",
        "                batch_size=batch_size,\n",
        "                num_beams=generation_config.num_beams,\n",
        "                device=inputs_tensor.device,\n",
        "                length_penalty=generation_config.length_penalty,\n",
        "                do_early_stopping=generation_config.early_stopping,\n",
        "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
        "                max_length=generation_config.max_length,\n",
        "            )\n",
        "            #print(f\"before: {input_ids.shape}\")\n",
        "            #print(input_ids)\n",
        "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
        "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "                input_ids=input_ids,\n",
        "                expand_size=generation_config.num_beams,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "            #print(f\"after: {input_ids.shape}\")\n",
        "            #print(input_ids)\n",
        "            # 13. run beam sample\n",
        "            result = _beam_search(\n",
        "                self,\n",
        "                input_ids,\n",
        "                beam_scorer,\n",
        "                logits_processor=prepared_logits_processor,\n",
        "                stopping_criteria=prepared_stopping_criteria,\n",
        "                generation_config=generation_config,\n",
        "                synced_gpus=synced_gpus,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Convert to legacy cache format if requested\n",
        "        if (\n",
        "            generation_config.return_legacy_cache is True\n",
        "            and not is_torchdynamo_compiling()\n",
        "            and hasattr(result, \"past_key_values\")\n",
        "            and getattr(result.past_key_values, \"to_legacy_cache\") is not None\n",
        "        ):\n",
        "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
        "        \"\"\"\n",
        "        return result"
      ],
      "metadata": {
        "id": "1uSBVAAyNa9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env TORCH_USE_CUDA_DSA=1 #doesnt work anymore due to a bug\n",
        "os.getenv(\"TORCH_USE_CUDA_DSA\"), os.getenv(\"CUDA_LAUNCH_BLOCKING\")"
      ],
      "metadata": {
        "id": "rzZgc86knXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEBUG Cell 2"
      ],
      "metadata": {
        "id": "Soqelf2mC5zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, TextStreamer\n",
        "from datasets import Dataset\n",
        "def generate_response():\n",
        "    #torch.set_printoptions(edgeitems=model.vocab_size//2)\n",
        "    dic = {\n",
        "      \"query\": [\n",
        "        \"Hi how are you doing. Please reason step by step, and put your final answer within \\boxed{}\",\n",
        "        \"What about solving an 2x + 3 = 7 equation? Please reason step by step, and put your final answer within \\boxed{}\",\n",
        "        \"What about solving an 3x + 5 = 7 equation? Please reason step by step, and put your final answer within \\boxed{}\",\n",
        "      ]\n",
        "    }\n",
        "    #messages = [\n",
        "    #    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation? Please reason step by step, and put your final answer within \\boxed{}\"},\n",
        "    #    {\"role\": \"user\", \"content\": \"What about solving an 3x + 5 = 7 equation? Please reason step by step, and put your final answer within \\boxed{}\"},\n",
        "    #]\n",
        "    def create_message_column(row):\n",
        "      messages = [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": row['query'],\n",
        "        }\n",
        "      ]\n",
        "      return {\"messages\": messages}\n",
        "    def format_dataset_chatml_and_tokenize(row):\n",
        "      return tokenizer.apply_chat_template(\n",
        "        row[\"messages\"],\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        add_special_tokens=False,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=512,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "      )\n",
        "    def tokenize(row):\n",
        "      return tokenizer(\n",
        "          row[\"chat\"],\n",
        "          add_special_tokens=False,\n",
        "          truncation=True,\n",
        "          padding=False,\n",
        "          max_length=512,\n",
        "          return_overflowing_tokens=False,\n",
        "          return_length=False,\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "    dataset = Dataset.from_dict(dic)\n",
        "    print(1,dataset[:])\n",
        "    dataset = dataset.map(create_message_column, remove_columns=dataset.column_names)\n",
        "    print(2,dataset[:])\n",
        "    dataset = dataset.map(format_dataset_chatml_and_tokenize, remove_columns=dataset.column_names)\n",
        "    print(3,dataset[:])\n",
        "    dataset = tokenizer.pad(dataset[:], return_tensors=\"pt\").to(\"cuda\")\n",
        "    print(4,dataset[:])\n",
        "\n",
        "    in_decoded = tokenizer.batch_decode(\n",
        "      dataset[\"input_ids\"],\n",
        "      skip_special_tokens=True,\n",
        "      clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "    prompt_lens = lens(in_decoded)\n",
        "    print(\"inputs decoded\")\n",
        "    pretty_print(in_decoded)\n",
        "    \"\"\"\n",
        "    genargs = dict(\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        num_beams=4,\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        max_time=180,\n",
        "        generation_config=model.generation_config,\n",
        "        use_cache=True, # Use caching for faster inference\n",
        "    )\n",
        "    \"\"\"\n",
        "    genargs = {\n",
        "        'max_new_tokens': 256,\n",
        "        'do_sample': True,\n",
        "        'num_beams': 4,\n",
        "        'temperature': 0.0012029803766213753,\n",
        "        'top_p': 0.0012730068237399462,\n",
        "        'top_k': None,\n",
        "        'max_time': 180,\n",
        "    }\n",
        "    genargs.update(\n",
        "      dict(\n",
        "        generation_config=model.generation_config,\n",
        "        use_cache=True, # Use caching for faster inference\n",
        "      )\n",
        "    )\n",
        "    \"\"\"\n",
        "    outputs = model.generate(\n",
        "        input_ids=dataset[\"input_ids\"],\n",
        "        attention_mask=dataset[\"attention_mask\"],\n",
        "        **genargs,\n",
        "    )\n",
        "    \"\"\"\n",
        "    outputs = generate_debug(\n",
        "        model,\n",
        "        input_ids=dataset[\"input_ids\"],\n",
        "        attention_mask=dataset[\"attention_mask\"],\n",
        "        **genargs,\n",
        "    )\n",
        "\n",
        "    out_decoded = tokenizer.batch_decode(\n",
        "      outputs,\n",
        "      skip_special_tokens=True,\n",
        "      clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "    print(\"outputs decoded\")\n",
        "    pretty_print(out_decoded)\n",
        "    answers = [out[prompt_len:] for prompt_len, out in zip(prompt_lens, out_decoded)]\n",
        "    print(\"answers\")\n",
        "    pretty_print(answers)\n",
        "#outputs = generate_response()"
      ],
      "metadata": {
        "id": "JbAtoEL0OruY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4mLOazXDPjp"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def clean_up_memory(*objs):\n",
        "  if len(objs) > 0:\n",
        "    logger.info(f\"Deleting objects:\\n{','.join(map(str,objs))}\")\n",
        "  for obj in objs:\n",
        "    del obj\n",
        "  logger.info(f\"Doing garbage collection\")\n",
        "  gc.collect()\n",
        "  gc.collect()\n",
        "  logger.info(f\"Emptying CUDA cache\")\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PS8mrs6PFu2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPL7arZBPNf0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"xiaobendanyn/tacred\"\n",
        "#dataset_name = \"./tacred\"\n",
        "\n",
        "dataset = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.column_names"
      ],
      "metadata": {
        "id": "1TXvHZ-b-b8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0jXn9Y1qofT"
      },
      "outputs": [],
      "source": [
        "dataset_sample = processor.sample_n(dataset, n=DATA_LENGTHS)\n",
        "train_dataset, val_dataset, test_dataset = processor.process(dataset_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyKAkheHWv6S"
      },
      "outputs": [],
      "source": [
        "clean_up_memory(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(train_dataset[:])"
      ],
      "metadata": {
        "id": "tNo6MGf1NwO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(val_dataset[:])"
      ],
      "metadata": {
        "id": "iLXPHzeKbvlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(test_dataset[:])"
      ],
      "metadata": {
        "id": "_gO_5fCWbwUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def merge_dicts(*dicts):\n",
        "  return {k: [d[k] for d in dicts if k in d] for k in {k for d in dicts for k in d}}\n",
        "\n",
        "class TorchData:\n",
        "  ops = {\n",
        "  \"add\": lambda a,b: a+b,\n",
        "  \"truediv\": lambda a,b: a/b,\n",
        "  }\n",
        "  defaults = {\n",
        "      \"device\": \"cpu\",\n",
        "  }\n",
        "  def __init__(self, _type, *args, **kwargs):\n",
        "    self.attrs = {}\n",
        "    for attr, val in TorchData.defaults.items():\n",
        "      self.attrs[attr] = kwargs.pop(attr, val)\n",
        "    if len(args) == 1:\n",
        "      _args = args[0]\n",
        "    elif _type == dict:\n",
        "      _args = kwargs\n",
        "    else:\n",
        "      _args = args\n",
        "    self._items = _type(_args)\n",
        "  def check_to(self, obj):\n",
        "    if isinstance(obj, (torch.Tensor, TorchData, BatchEncoding)):\n",
        "      return obj.to(self.device)\n",
        "    return obj\n",
        "  def __len__(self):\n",
        "    return len(self._items)\n",
        "  def __repr__(self):\n",
        "    return f\"{self.__class__.__name__}({self._items}, {', '.join([f'{k}={v}' for k, v in self.attrs.items()])})\"\n",
        "  def __getattr__(self, attr):\n",
        "    if hasattr(self._items, attr):\n",
        "      return getattr(self._items, attr)\n",
        "    elif attr in self.attrs:\n",
        "      return self.attrs[attr]\n",
        "    raise AttributeError()\n",
        "  def __setattr__(self, attr, val):\n",
        "    if attr in TorchData.defaults:\n",
        "      self.attrs[attr] = val\n",
        "    else:\n",
        "      super().__setattr__(attr, val)\n",
        "  def __iter__(self):\n",
        "    return iter(self._items)\n",
        "  def __getitem__(self, i):\n",
        "    return self._items[i]\n",
        "  def __setitem__(self, i, v):\n",
        "    self._items[i] = v\n",
        "  # Add custom operators here\n",
        "  def __add__(self, other):\n",
        "    return self.__class__.apply(TorchData.ops[\"add\"], self, other)\n",
        "  def __radd__(self, other):\n",
        "    return self.__class__.apply(TorchData.ops[\"add\"], other, self)\n",
        "  def __truediv__(self, other):\n",
        "    return self.__class__.apply(TorchData.ops[\"truediv\"], self, other)\n",
        "  def __rtruediv__(self, other):\n",
        "    return self.__class__.apply(TorchData.ops[\"truediv\"], other, self)\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, TorchData):\n",
        "      return self._items == other._items\n",
        "    return self._items == other\n",
        "  def remove(self, *args):\n",
        "    self.remove_with(lambda k: k in args)\n",
        "class TorchList(TorchData):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(list, *args, **kwargs)\n",
        "  def to(self, device):\n",
        "    self.device = device\n",
        "    return TorchList(map(self.check_to, self._items), **self.attrs)\n",
        "  @classmethod\n",
        "  def apply(cls, operator, operand1, operand2):\n",
        "    def aggregate(ops):\n",
        "      return operator(*ops)\n",
        "    if isinstance(operand1, cls) and isinstance(operand2, Numeric):\n",
        "      operand2 = cls([operand2]*len(operand1))\n",
        "    elif isinstance(operand1, Numeric) and isinstance(operand2, cls):\n",
        "      operand1 = cls([operand1]*len(operand2))\n",
        "    elif not (isinstance(operand1, (Numeric, cls)) or isinstance(operand2, (Numeric, cls))):\n",
        "      raise NotImplementedError()\n",
        "    return TorchList(map(aggregate, zip(operand1, operand2)), **operand1.attrs)\n",
        "  def remove_with(self, condition):\n",
        "    self._items = [k for k in self._items if not condition(k)]\n",
        "  @classmethod\n",
        "  def merge(cls, operand1, operand2):\n",
        "    return TorchList(operand1._items + operand2._items, **operand1.attrs)\n",
        "class TorchDict(TorchData):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(dict, *args, **kwargs)\n",
        "  def to(self, device):\n",
        "    self.device = device\n",
        "    return TorchDict({k: self.check_to(v) for k,v in self.items()}, **self.attrs)\n",
        "  @classmethod\n",
        "  def apply(cls, operator, operand1, operand2):\n",
        "    def aggregate(kvpairs):\n",
        "      kvpair = tuple(zip(*kvpairs))\n",
        "      return (kvpair[0][0], operator(*kvpair[1]))\n",
        "    if isinstance(operand1, cls) and isinstance(operand2, Numeric):\n",
        "      operand2 = cls(zip(operand1.keys(), [operand2]*len(operand1)))\n",
        "    elif isinstance(operand1, Numeric) and isinstance(operand2, cls):\n",
        "      operand1 = cls(zip(operand2.keys(), [operand1]*len(operand2)))\n",
        "    elif not (isinstance(operand1, (Numeric, cls)) or isinstance(operand2, (Numeric, cls))):\n",
        "      raise NotImplementedError()\n",
        "    return TorchDict(map(aggregate, zip(operand1.items(), operand2.items())), **operand1.attrs)\n",
        "  @classmethod\n",
        "  def merge(cls, operand1, operand2):\n",
        "    tmp = TorchDict(operand1._items, **operand1.attrs)\n",
        "    tmp.update(operand2._items)\n",
        "    return tmp\n",
        "  def remove_with(self, condition):\n",
        "    self._items = {k: v for k,v in self._items.items() if not condition(k)}\n",
        "@dataclass\n",
        "class CustomDefaultCollator:\n",
        "  tokenizer: PreTrainedTokenizerBase\n",
        "  pad_to_multiple_of: Optional[int] = None\n",
        "  def __call__(\n",
        "      self,\n",
        "      examples: List[Union[List[int], Any, Dict[str, Any]]],\n",
        "      custom_fields = [\"labels\"],\n",
        "    ) -> Dict[str, Any]:\n",
        "    # Separate token fields and custom fields, custom fields wont be padded\n",
        "    token_fields_rows_list = [{k: row[k] for k in row if k not in custom_fields} for row in examples]\n",
        "    custom_fields_rows_list = [{k: row[k] for k in row if k in custom_fields} for row in examples]\n",
        "    # Pad token fields\n",
        "    batch = self.tokenizer.pad(token_fields_rows_list, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
        "    # Merge padded token fields and custom fields\n",
        "    custom_fields_rows_list = merge_dicts(*custom_fields_rows_list)\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "    if self.tokenizer.pad_token_id is not None:\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch if (custom_fields_rows_list == {}) else TorchList([batch, custom_fields_rows_list])\n",
        "\n",
        "#loader = DataLoader(\n",
        "#  val_dataset,\n",
        "#  batch_size = 8,\n",
        "#  collate_fn = CustomDefaultCollator(processor.tokenizer),\n",
        "#  num_workers = 0,\n",
        "#  pin_memory = True,\n",
        "#  persistent_workers = False,\n",
        "#)\n",
        "#for elem in loader:\n",
        "#  logger.debug(elem)"
      ],
      "metadata": {
        "id": "OaLKp2t4O2P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer API"
      ],
      "metadata": {
        "id": "aNSZo1DLJWPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBsoIsP-Dev0"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.utils import logging\n",
        "from torchmetrics import Metric\n",
        "from torchmetrics.text import BLEUScore, ROUGEScore\n",
        "from transformers import get_scheduler, DataCollatorForLanguageModeling, BatchEncoding\n",
        "from peft import get_peft_model, PeftModel, LoftQConfig, LoraConfig, TaskType, prepare_model_for_kbit_training, replace_lora_weights_loftq\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from peft.utils.integrations import gather_params_ctx\n",
        "import copy\n",
        "import multiprocessing as mp\n",
        "\n",
        "class CustomMetric:\n",
        "  def forward(self, raw_text_preds, raw_text_true):\n",
        "    raise NotImplementedError()\n",
        "  def __call__(self, preds, true_labels):\n",
        "    length = len(preds)\n",
        "    if len(true_labels) != length:\n",
        "      raise ValueError(f\"true labels and predictions have different lengths: {len(true_labels)} != {length}\")\n",
        "\n",
        "    def proc_pair(start, end):\n",
        "      for index in range(start, end):\n",
        "        raw_preds = preds[index]\n",
        "        raw_true = true_labels[index]\n",
        "\n",
        "      return self.forward(raw_text_preds, raw_text_true)\n",
        "    max_procs = os.cpu_count()\n",
        "    proc_length = length // max_procs + 1\n",
        "    procs = [mp.Process(target=proc_pair, args=(i, max(i+proc_length, length))) for i in range(0,length,proc_length)]\n",
        "\n",
        "    index = 0\n",
        "    for index in range(length):\n",
        "      raw_text_preds = preds[index]\n",
        "      raw_text_true = true_labels[index]\n",
        "      try:\n",
        "        raw_text_preds, raw_text_true = self.forward(raw_text_preds, raw_text_true)\n",
        "        fixed = False\n",
        "      except Exception:\n",
        "        #logger.debug(f\"Sample {index}\")\n",
        "        #logger.debug(f\"True text: {raw_text_true}\")\n",
        "        #logger.debug(f\"Predicted text: {raw_text_preds}\")\n",
        "        pass\n",
        "    return self.ret()\n",
        "class MicroF1(CustomMetric):\n",
        "  def __init__(self):\n",
        "    self.tps = 0\n",
        "    self.total = 0\n",
        "  def forward(self, raw_text_preds, raw_text_true):\n",
        "    raw_text_preds = raw_text_preds.strip(\"` \\n\")\n",
        "    if raw_text_preds.startswith(\"python\"):\n",
        "      raw_text_preds = self.fix(raw_text_preds)\n",
        "    preds_triples = eval(raw_text_preds)\n",
        "    true_triples = eval(raw_text_true)\n",
        "    preds_triples_set = set(preds_triples)\n",
        "    true_triples_set = set(true_triples)\n",
        "    true_positives = preds_triples_set & true_triples_set # intersection set\n",
        "    lentp = len(true_positives)\n",
        "    self.tps += lentp # add intersection cardinal\n",
        "    self.total += (len(preds_triples) + len(true_triples) - lentp) # add union cardinal\n",
        "    return raw_text_preds, raw_text_true\n",
        "  def ret(self):\n",
        "    if self.total == 0:\n",
        "      return 0\n",
        "    return self.tps / self.total\n",
        "  def fix(self, raw_text_preds):\n",
        "    return raw_text_preds[raw_text_preds.index(\"[\"):]\n",
        "@dataclass\n",
        "class DataArg:\n",
        "  fullname: str\n",
        "  alias: str\n",
        "  inference_mode: str\n",
        "  dataset: Mapping\n",
        "  batch_func: Callable\n",
        "  batch_size: int = 0\n",
        "  loader: DataLoader = None\n",
        "@dataclass\n",
        "class CustomArg:\n",
        "  name: str\n",
        "  args: dict\n",
        "@dataclass\n",
        "class OptimArg(CustomArg):\n",
        "  pass\n",
        "@dataclass\n",
        "class SchedulerArg(CustomArg):\n",
        "  pass\n",
        "class ReinforceTrainer:\n",
        "  def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        test_dataset,\n",
        "        **kwargs,\n",
        "    ):\n",
        "    self.init_params()\n",
        "    self.adapter_name = \"tacred\"\n",
        "    self.writer = SummaryWriter()\n",
        "    self.tokenizer = tokenizer\n",
        "    #self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
        "    self.data_collator = CustomDefaultCollator(tokenizer=self.tokenizer)\n",
        "    self.update(\n",
        "      model,\n",
        "      train_dataset,\n",
        "      val_dataset,\n",
        "      test_dataset,\n",
        "      **kwargs\n",
        "    )\n",
        "  def __getattr__(self, attr):\n",
        "    if attr == \"defaults\":\n",
        "      return {}\n",
        "    elif attr in self.params:\n",
        "      return self.params[attr]\n",
        "    else:\n",
        "      raise AttributeError\n",
        "  def __setattr__(self, attr, val):\n",
        "    if attr in self.defaults:\n",
        "      self.params[attr] = val\n",
        "    else:\n",
        "      super().__setattr__(attr, val)\n",
        "  def init_update(\n",
        "        self,\n",
        "        kwargs,\n",
        "        update_params_dic,\n",
        "        callback_update=Identity(),\n",
        "        update_kwargs={},\n",
        "        callback_init=Identity(),\n",
        "        init_kwargs={},\n",
        "      ):\n",
        "    self.defaults.update(update_params_dic)\n",
        "    #logger.debug(kwargs, self.params, self.defaults)\n",
        "    def update_if_nondefault(attr, val):\n",
        "      val2set = kwargs.get(attr, val)\n",
        "      #if attr in update_params_dic and attr == \"lora_alpha\":\n",
        "      #  cond = hasattr(self, attr)\n",
        "      #  logger.debug(\"lora_alpha\", cond, val2set)\n",
        "      #  if cond:\n",
        "      #    logger.debug(getattr(self, attr))\n",
        "      if notexists(self, attr):\n",
        "        self.params[attr] = val2set\n",
        "        return True\n",
        "      return False\n",
        "    def update_via_list():\n",
        "      tmplist = [update_if_nondefault(*attr_val) for attr_val in update_params_dic.items()]\n",
        "      logger.debug(f\"Updated params: {dict(zip(update_params_dic.keys(), tmplist))}\")\n",
        "      return any(tmplist)\n",
        "    if update_via_list():\n",
        "      logger.info(f\"Running init {callback_init.__name__}\")\n",
        "      callback_init(**init_kwargs)\n",
        "    else:\n",
        "      logger.info(f\"Running update {callback_update.__name__}\")\n",
        "      callback_update(**update_kwargs)\n",
        "  def update_params(self, **kwargs):\n",
        "    self.params.update(kwargs)\n",
        "  def init_params(self, **kwargs):\n",
        "    self.defaults = {\n",
        "        \"num_epochs\": 1,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        # EVAL\n",
        "        \"calc_loss_on_eval\": False,\n",
        "        # LOSS FUNCTION\n",
        "        \"shift_labels\": True,\n",
        "        \"baseline\": True,\n",
        "        \"label_smoothing_factor\": 0.1,#\n",
        "        \"train_metric\": None,#\n",
        "        \"train_metric_key\": None,#\n",
        "        \"eval_metric\": [MicroF1()],\n",
        "        \"num_samples\": 0,#\n",
        "        \"multiple\": True,#\n",
        "        # GEN KWARGS\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"do_sample\": True,\n",
        "        \"num_beams\": 1,#\n",
        "        \"temperature\": 0.3,#\n",
        "        \"top_p\": 0.95,#\n",
        "        \"max_time\": 180,\n",
        "    }\n",
        "    self.params = {k: kwargs.get(k, v) for k, v in self.defaults.items()}\n",
        "  def reset_model_params(self):\n",
        "    # to reset all params\n",
        "    #self.model.init_weights()\n",
        "    # to reset only lora params\n",
        "    def is_in_module(key, module):\n",
        "      for name, _ in module.named_children():\n",
        "        if key in name:\n",
        "          return True\n",
        "      return False\n",
        "    def reset_lora_weights(module):\n",
        "      init_lora_weights = self.lora_config.init_lora_weights\n",
        "      if isinstance(init_lora_weights, str) and init_lora_weights.startswith(\"pissa\"):\n",
        "        with gather_params_ctx(module.get_base_layer().weight):\n",
        "          module.pissa_init(self.adapter_name, init_lora_weights)\n",
        "      elif isinstance(init_lora_weights, str) and init_lora_weights.startswith(\"corda\"):\n",
        "        with gather_params_ctx(module.get_base_layer().weight):\n",
        "          module.corda_init(self.adapter_name, init_lora_weights)\n",
        "      elif isinstance(init_lora_weights, str) and init_lora_weights.lower() == \"olora\":\n",
        "        with gather_params_ctx(module.get_base_layer().weight):\n",
        "          module.olora_init(self.adapter_name)\n",
        "      elif init_lora_weights == \"loftq\":\n",
        "        with gather_params_ctx(module.get_base_layer().weight):\n",
        "          module.loftq_init(self.adapter_name)\n",
        "      elif init_lora_weights == \"eva\":\n",
        "        nn.init.zeros_(module.lora_B[self.adapter_name].weight)\n",
        "      elif init_lora_weights:\n",
        "        module.reset_lora_parameters(self.adapter_name, init_lora_weights)\n",
        "    def reset_if_lora(name, module):\n",
        "      if is_in_module(\"lora\", module):\n",
        "        logger.debug(name)\n",
        "        reset_lora_weights(module)\n",
        "    logger.debug(\"Reset parameters\")\n",
        "    map_named_modules(reset_if_lora, model)\n",
        "  def init_model(self, model, **kwargs):\n",
        "    if hasattr(self, \"model\") and isinstance(self.model, PeftModel):\n",
        "      logger.info(f\"Unloading the model\")\n",
        "      if model == None:\n",
        "        model = self.model.unload()\n",
        "    # Enable gradient checkpointing to save memory\n",
        "    _model = prepare_model_for_kbit_training(\n",
        "        model,\n",
        "        use_gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": True}\n",
        "    )\n",
        "    self.lora_config = LoraConfig(\n",
        "      r=self.lora_r,\n",
        "      lora_alpha=self.lora_alpha,\n",
        "      lora_dropout=self.lora_dropout,\n",
        "      task_type=TaskType.CAUSAL_LM,\n",
        "      target_modules=\"all-linear\",# list of the modules that should be targeted by LoRA, all linear layers\n",
        "      use_rslora=True, # Rank-Stabilized LoRA: scale by lora_r/sqrt(lora_alpha) instead of lora_r/lora_alpha\n",
        "      use_dora=True,\n",
        "    )\n",
        "\n",
        "    self.model = get_peft_model(_model, self.lora_config, adapter_name = self.adapter_name)\n",
        "    # Lora weights are only used for attention and mlp weights\n",
        "    # Below takes too long, probably longer with more lora weights\n",
        "    #replace_lora_weights_loftq(self.model)\n",
        "  def init_optimizer(self, **kwargs):\n",
        "    self.optimizer = getattr(optim, self.optim_arg.name)(self.model.parameters(), **self.optim_arg.args)\n",
        "  def init_scheduler(self, **kwargs):\n",
        "    self.scheduler = get_scheduler(\n",
        "       self.scheduler_arg.name,\n",
        "       optimizer=self.optimizer,\n",
        "       **self.scheduler_arg.args,\n",
        "    )\n",
        "  def get_loader(self, dataarg):\n",
        "    return DataLoader(\n",
        "      dataarg.dataset,\n",
        "      batch_size = dataarg.batch_size,\n",
        "      collate_fn = self.data_collator,\n",
        "      num_workers = 0,\n",
        "      pin_memory = True,\n",
        "      persistent_workers = False,\n",
        "    )\n",
        "  def init_loaders(self, train_dataset, val_dataset, test_dataset):\n",
        "    self.dataargs = {\n",
        "        \"train\": DataArg(\"training\",\"train\", \"train\", train_dataset, self.train_on_batch, self.train_batch_size),\n",
        "        \"val\": DataArg(\"validation\",\"val\", \"eval\", val_dataset, self.eval_on_batch, self.val_batch_size),\n",
        "        \"test\": DataArg(\"test\",\"test\", \"eval\", test_dataset, self.eval_on_batch, self.test_batch_size),\n",
        "    }\n",
        "    for name, dataarg in self.dataargs.items():\n",
        "      dataarg.loader = self.get_loader(dataarg)\n",
        "  def update(\n",
        "      self,\n",
        "      model=None,\n",
        "      train_dataset=None,\n",
        "      val_dataset=None,\n",
        "      test_dataset=None,\n",
        "      **kwargs,\n",
        "    ):\n",
        "    batch_size = kwargs.get(\"batch_size\", 8)\n",
        "    self.init_update(\n",
        "      kwargs,\n",
        "      {\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"val_batch_size\": batch_size,\n",
        "          \"test_batch_size\": batch_size,\n",
        "      },\n",
        "      callback_init = self.init_loaders,\n",
        "      init_kwargs = {\n",
        "        \"train_dataset\": train_dataset,\n",
        "        \"val_dataset\": val_dataset,\n",
        "        \"test_dataset\": test_dataset,\n",
        "      }\n",
        "    )\n",
        "    self.init_update(\n",
        "      kwargs,\n",
        "      {\n",
        "        \"lora_alpha\": 32,\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_dropout\": 0.05,\n",
        "      },\n",
        "      callback_init = self.init_model,\n",
        "      callback_update = self.reset_model_params,\n",
        "      init_kwargs = {\"model\": model},\n",
        "    )\n",
        "    self.init_update(\n",
        "      kwargs,\n",
        "      {\n",
        "        \"optim_arg\": OptimArg(name=\"AdamW\", args={\"lr\": 1e-8, \"betas\": (0.9, 0.999), \"weight_decay\": 1e-2}),\n",
        "      },\n",
        "      callback_init = self.init_optimizer,\n",
        "      callback_update = self.init_optimizer,\n",
        "    )\n",
        "    self.init_update(\n",
        "      kwargs,\n",
        "      {\n",
        "        \"scheduler_arg\": SchedulerArg(\n",
        "          name=\"linear\",\n",
        "          args={\n",
        "            \"num_training_steps\": kwargs.get(\"num_epochs\", self.num_epochs) * len(self.dataargs[\"train\"].loader),\n",
        "            \"num_warmup_steps\": 0,\n",
        "          }\n",
        "        ),\n",
        "      },\n",
        "      callback_init = self.init_scheduler,\n",
        "      callback_update = self.init_scheduler,\n",
        "    )\n",
        "    self.update_params(**kwargs)\n",
        "    logger.debug(f\"Default params:\\n {self.defaults}\")\n",
        "    logger.info(f\"Current params:\\n {self.params}\")\n",
        "  def remove_padding_mask(self, labels):\n",
        "    padding_mask = labels < 0 # (N, L)\n",
        "    return padding_mask, labels.masked_fill_(padding_mask, self.tokenizer.pad_token_id)\n",
        "  def inference(self, batch):\n",
        "    labels = batch.pop(\"labels\")\n",
        "    if self.dataarg.inference_mode == \"eval\":\n",
        "      with torch.inference_mode():\n",
        "        outputs = self.model(\n",
        "            batch.input_ids,\n",
        "            batch.attention_mask,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    else:\n",
        "      outputs = self.model(\n",
        "          batch.input_ids,\n",
        "          batch.attention_mask,\n",
        "          use_cache=True,\n",
        "      )\n",
        "    logits = outputs.logits\n",
        "    return logits, labels\n",
        "  def loss_function(self, batch):\n",
        "    \"\"\"params should be (metric==\"ce\" and num_samples==0) or (metric!=\"ce\" and num_samples>0)\"\"\"\n",
        "    def calc_scale(candidates, references):\n",
        "      candidates_detokenized = self.tokenizer.batch_decode(\n",
        "        candidates,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "      )\n",
        "      references_detokenized = self.tokenizer.batch_decode(\n",
        "        references,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "      )\n",
        "      def ret_metric(res):\n",
        "        return res if self.train_metric_key==None else res[self.train_metric_key]\n",
        "      if self.multiple:\n",
        "        res = [ret_metric(self.train_metric([c], [references_detokenized])) for c in candidates_detokenized]\n",
        "      else:\n",
        "        res = [ret_metric(self.train_metric([c], [[r]])) for c, r in zip(candidates_detokenized, references_detokenized)]\n",
        "      return torch.tensor(res).to(self.model.device)\n",
        "    logits, labels = self.inference(batch)\n",
        "    if self.shift_labels:\n",
        "        logits = logits[..., :-1, :].contiguous()# (N, L, S)\n",
        "        labels = labels[..., 1:].contiguous()# (N, L)\n",
        "    # log probs\n",
        "    log_probs = -F.log_softmax(logits, dim=-1) # (N, L, S)\n",
        "    # padding mask\n",
        "    padding_mask, _ = self.remove_padding_mask(labels) # (N, L)\n",
        "    num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
        "    # smoothed loss\n",
        "    smoothed_loss = log_probs.sum(dim=-1) # (N, L)\n",
        "    smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
        "    smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
        "    lpa = None\n",
        "    metric_adv = None\n",
        "    if self.num_samples < 0:\n",
        "      raise ValueError(f\"Number of samples {self.num_samples} cannot be negative\")\n",
        "    cond1ce = isinstance(self.train_metric, (nn.CrossEntropyLoss, NoneType))\n",
        "    cond2ce = isinstance(self.train_metric, str) and self.train_metric.lower() in (\"ce\", \"crossentropyloss\")\n",
        "    if cond1ce or cond2ce:\n",
        "      if self.num_samples > 0:\n",
        "        raise ValueError(\"Cross entropy loss cannot be used with num_samples\")\n",
        "      metric_adv = 1 # N\n",
        "      labels_index = labels.view(*labels.shape, 1) # (N, L, 1)\n",
        "      lpa = torch.gather(log_probs, index=labels_index, dim=-1).squeeze(-1) # (N, L)\n",
        "    elif isinstance(self.train_metric, ROUGEScore) and self.train_metric_key == None:\n",
        "      raise ValueError(\"ROUGEScore requires metric_key\")\n",
        "    elif isinstance(self.train_metric, Metric):# Other metrics\n",
        "      actions_amax = logits.argmax(dim=-1) # (N, L)\n",
        "      scale_amax = calc_scale(actions_amax, labels) if self.baseline else 0 # N\n",
        "      scale_sample = None\n",
        "      if self.num_samples > 0:\n",
        "        probs = F.softmax(logits, dim=-1) # (N, L, S)\n",
        "        probs_view_2d = probs.view(-1, logits.shape[-1]) # (N*L, S)\n",
        "\n",
        "        actions_sample_2d = torch.multinomial(probs_view_2d, self.num_samples, replacement=True) # (N*L, R)\n",
        "        actions_sample_2d_T = actions_sample_2d.T # (R, N*L)\n",
        "        actions_sample = actions_sample_2d_T.reshape(-1, labels.shape[-1]) # (R*N, L)\n",
        "\n",
        "        labels_sample = labels.expand(self.num_samples, *labels.shape).reshape(-1, labels.shape[-1]) # (R*N, L)\n",
        "        scale_sample = calc_scale(actions_sample, labels_sample).view(self.num_samples,-1) # (R, N)\n",
        "        # -(Q - b)\n",
        "        metric_adv = scale_amax - scale_sample # (R,N)\n",
        "        actions_sample_index = actions_sample.view(self.num_samples, *labels.shape,1) # (R, N, L, 1)\n",
        "        log_probs_expanded = log_probs.expand(self.num_samples, *log_probs.shape) # (R, N, L, S)\n",
        "        lpa = torch.gather(log_probs_expanded, index=actions_sample_index, dim=-1).squeeze(-1) # (R, N, L)\n",
        "        num_active_elements *= self.num_samples\n",
        "      else:\n",
        "        raise ValueError(\"Metrics other than cross-entropy require num_samples > 0\")\n",
        "    else:\n",
        "      raise ValueError(\"Metrics other than cross-entropy should be from torchmetrics library\")\n",
        "    lpa.masked_fill_(padding_mask, 0.0) # (R, N, L) or (N, L)\n",
        "    lpa_len_1 = len(lpa.shape)-1\n",
        "    lpa_permuted = lpa.permute(lpa_len_1, *range(lpa_len_1)) # (L, R, N) or (L, N)\n",
        "    loss = metric_adv * lpa_permuted # (L, R, N) or (L, N)\n",
        "    loss = loss.sum() / num_active_elements\n",
        "    return (1 - self.label_smoothing_factor) * loss + self.label_smoothing_factor * smoothed_loss\n",
        "  def predict_and_score(self, batch, labels):\n",
        "    true_labels = labels[\"labels\"]\n",
        "    preds, pred_time = self.exec_time(self.predict, batch)\n",
        "    scores = self.exec_time(self.score, preds, true_labels, timer_name = \"score time\")\n",
        "    scores[\"predict time\"] = pred_time\n",
        "    return scores\n",
        "  def score(self, preds, true_labels):\n",
        "    return TorchDict({metric.__class__.__name__.lower(): metric(preds, true_labels) for metric in self.eval_metric})\n",
        "  def predict(self, batch):\n",
        "    logger.debug(f\"batch 2: {batch}\")\n",
        "    logger.debug(f\"model device 2: {self.model.device}\")\n",
        "\n",
        "    input_ids = batch.input_ids\n",
        "    attention_mask = batch.attention_mask\n",
        "    in_decoded = tokenizer.batch_decode(\n",
        "      input_ids,\n",
        "      skip_special_tokens=True,\n",
        "      clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "    prompt_lens = lens(in_decoded)\n",
        "    #logger.debug(f\"inputs decoded:\")\n",
        "    #pretty_print(in_decoded)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        max_new_tokens=self.max_new_tokens,\n",
        "        do_sample=self.do_sample,\n",
        "        num_beams=self.num_beams,\n",
        "        temperature=self.temperature,\n",
        "        top_p=self.top_p,\n",
        "        max_time = self.max_time,\n",
        "        generation_config=model.generation_config,\n",
        "        use_cache=True, # Use caching for faster inference\n",
        "    )\n",
        "    output = self.model.generate(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      **generate_kwargs,\n",
        "    )\n",
        "    out_decoded = self.tokenizer.batch_decode(\n",
        "      output,\n",
        "      skip_special_tokens=True,\n",
        "      clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "    #logger.debug(\"outputs decoded\")\n",
        "    #pretty_print(out_decoded)\n",
        "    return [out[prompt_len+1:] for prompt_len, out in zip(prompt_lens, out_decoded)]\n",
        "  def backward(self, loss):\n",
        "    if (self.batch_step + 1) % self.gradient_accumulation_steps == 0:\n",
        "        torch.autograd.backward(loss)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "  def train_on_batch(self):\n",
        "    loss, loss_time = self.exec_time(self.loss_function, self.batch)\n",
        "    loss /= self.gradient_accumulation_steps\n",
        "    _, backward_time = self.exec_time(self.backward, loss)\n",
        "    return TorchDict({\"loss\": torch.tensor(loss.item()), \"loss time\": loss_time, \"backward time\": backward_time})\n",
        "  def eval_on_batch(self):\n",
        "    batch, labels = self.batch\n",
        "    scores = self.predict_and_score(batch, labels)\n",
        "    if self.calc_loss_on_eval:\n",
        "      loss, loss_time = self.exec_time(self.loss_function, batch)\n",
        "      scores[\"loss\"] = loss\n",
        "      scores[\"loss time\"] = loss_time\n",
        "    if scores == {}:\n",
        "      raise ValueError(f\"Scores cannot be empty\")\n",
        "    return scores\n",
        "  def process_batch(self):\n",
        "    #_model_old = copy.deepcopy(self.model)\n",
        "    self.batch = self.batch.to(self.model.device)\n",
        "    logger.debug(f\"Batch: {self.batch}\")\n",
        "    batch_callback = self.dataarg.batch_func\n",
        "    res = batch_callback()\n",
        "    #logger.debug(self.scheduler.get_last_lr())\n",
        "    #compare_models(_model_old, self.model)\n",
        "    return res\n",
        "  def exec_time(self, func, *args, **kwargs):\n",
        "    timer_name = kwargs.pop(\"timer_name\", \"time\")\n",
        "    start = time.time()\n",
        "    res = func(*args, **kwargs)\n",
        "    duration = time.time() - start\n",
        "    if isinstance(res, (dict, TorchDict)):\n",
        "      res[timer_name] = duration\n",
        "      return res\n",
        "    return res, duration\n",
        "  def get_iterator(self, dataarg):\n",
        "    prefix = \"\" if notexists(self, \"epoch\") else f\"Epoch {self.epoch} \"\n",
        "    desctxt = f\"{prefix}{dataarg.fullname} batches\"\n",
        "    return logging.tqdm(dataarg.loader, desc=desctxt)\n",
        "  def init_batches(self, data_split):\n",
        "    if data_split not in self.dataargs:\n",
        "      raise ValueError(f\"Unknown value for data_split: {data_split}\")\n",
        "    self.dataarg = self.dataargs[data_split]\n",
        "    self.num_batches = len(self.dataarg.loader)\n",
        "    self.iterator = enumerate(self.get_iterator(self.dataarg), start=1)\n",
        "    getattr(self.model, self.dataarg.inference_mode)()\n",
        "    logger.info(f\"Activated {self.dataarg.inference_mode} mode, training mode is {self.model.training}\")\n",
        "  def loop_batches(self, data_split):\n",
        "    self.init_batches(data_split)\n",
        "\n",
        "    #if logger.is_verbosity(logging.DEBUG) and data_split == \"val\":\n",
        "    #  self.device_old = self.model.device\n",
        "    #  self.model.to_full_precision()\n",
        "    #  self.model = self.model.to(\"cpu\")\n",
        "    #  self.model.to_empty_cache()\n",
        "    mean_scores = 0\n",
        "    for self.batch_step, self.batch in self.iterator:\n",
        "      scores = self.process_batch()\n",
        "      self.print_results(scores)\n",
        "      mean_scores += scores\n",
        "    self.batch_step = None\n",
        "\n",
        "    if exists(self, \"device_old\"):\n",
        "      self.model.to_full_precision()\n",
        "      self.model = self.model.to(self.device_old)\n",
        "      self.model.to_empty_cache()\n",
        "      del self.device_old\n",
        "    return mean_scores / self.num_batches\n",
        "  def print_results(self, scores):\n",
        "    prefix = \"Mean\"\n",
        "    results_dict = {}\n",
        "    if exists(self, \"epoch\"):\n",
        "      prefix = \"Batch mean\"\n",
        "      results_dict[\"epoch\"] = self.epoch\n",
        "    if exists(self, \"batch_step\"):\n",
        "      prefix = \"Batch step\"\n",
        "      results_dict[\"batch\"] = self.batch_step\n",
        "      if exists(self, \"epoch\"):\n",
        "        global_step = (self.epoch - 1) * self.num_batches + self.batch_step\n",
        "        results_dict[\"global\"] = global_step\n",
        "    results_dict.update(scores)\n",
        "    results_txt = f\"{prefix} {self.dataarg.fullname} {', '.join([f'{k}: {v}' for k, v in results_dict.items()])}\"\n",
        "    logger.info(results_txt)\n",
        "    scores.remove_with(lambda k: k.lower().endswith(\"time\"))\n",
        "  def process_batch_once(self, data_split):\n",
        "    if data_split not in self.dataargs:\n",
        "     raise ValueError(f\"Unknown value for data split: {data_split}. Data split should be one of {tuple(self.dataargs.keys())}\")\n",
        "    if not hasattr(self, \"batch_step\") or self.batch_step in (1, None):\n",
        "      self.init_batches(data_split)\n",
        "    scores = None\n",
        "    try:\n",
        "      self.batch_step, self.batch = next(self.iterator)\n",
        "      scores = self.process_batch()\n",
        "      self.print_results(scores)\n",
        "    except StopIteration:\n",
        "      self.batch_step = None\n",
        "    return scores\n",
        "  def train_loop(self):\n",
        "    mean_train_loss = 0\n",
        "    for self.epoch in logging.tqdm(range(1,self.num_epochs+1), desc=f\"Training epochs\"):\n",
        "      batch_train_loss = self.exec_time(self.loop_batches, \"train\", timer_name=\"batch time\")\n",
        "      self.print_results(batch_train_loss)\n",
        "      mean_train_loss += batch_train_loss\n",
        "    self.epoch = None\n",
        "    mean_train_loss /= self.num_epochs\n",
        "    return mean_train_loss\n",
        "  def train(self):\n",
        "    mean_train_loss = self.exec_time(self.train_loop, timer_name=\"total time\")\n",
        "    self.print_results(mean_train_loss)\n",
        "    val_scores = self.exec_time(self.loop_batches, \"val\", timer_name=\"total time\")\n",
        "    self.print_results(val_scores)\n",
        "    clean_up_memory()\n",
        "    return mean_train_loss, val_scores\n",
        "  def __del__(self):\n",
        "    logger.info(f\"Deleting {self.__class__.__name__} object\")\n",
        "    self.writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXtj5xQoCFeh"
      },
      "outputs": [],
      "source": [
        "trainer = ReinforceTrainer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_info_params(\n",
        "    trainer.model,\n",
        "    size_type_to_print=(\"GB\", \"MB\"),\n",
        "    print_trainable=\"a\",\n",
        "    calc_param_counts = False,\n",
        "    calc_grad_counts = False,\n",
        ")"
      ],
      "metadata": {
        "id": "lCJjHAeVHyvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = trainer.train()\n",
        "res"
      ],
      "metadata": {
        "id": "cKjQICGyZVop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPSILON = 1e-4\n",
        "\n",
        "def objective(trial):\n",
        "  train_metric_key = trial.suggest_categorical(\"train_metric_key\", [None, 'rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure', 'rougeLsum_fmeasure'])\n",
        "  if train_metric_key == None:\n",
        "    num_samples = trial.suggest_int(\"num_samples\", 0, 4)\n",
        "    if num_samples == 0:\n",
        "      train_metric = None\n",
        "    else:\n",
        "      train_metric = BLEUScore(n_gram=4, smooth=True)\n",
        "  else:\n",
        "    num_samples = trial.suggest_int(\"num_samples\", 1, 4)\n",
        "    train_metric = ROUGEScore(use_stemmer=True, accumulate='avg')\n",
        "\n",
        "  beta1 = trial.suggest_float(\"beta1\", 0.5, 1-EPSILON, log=True)\n",
        "  beta2 = trial.suggest_float(\"beta2\", beta1, 1-EPSILON, log=True)\n",
        "  params = {\n",
        "    # LOSS KWARGS\n",
        "    \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", EPSILON, 1, log=True),#\n",
        "    \"train_metric\": train_metric,#\n",
        "    \"train_metric_key\": train_metric_key,#\n",
        "    \"num_samples\": num_samples,#\n",
        "    \"multiple\": trial.suggest_categorical(\"multiple\", [True, False]),#\n",
        "    # GEN KWARGS\n",
        "    \"do_sample\": trial.suggest_categorical(\"do_sample\", [True, False]),#\n",
        "    \"num_beams\": trial.suggest_int(\"num_samples\", 1, 4),\n",
        "    \"temperature\": trial.suggest_float(\"temperature\", EPSILON, 2, log=True),\n",
        "    \"top_p\": trial.suggest_float(\"top_p_1\", EPSILON, 1, log=True),\n",
        "    # MODEL KWARGS\n",
        "    \"lora_r\": trial.suggest_int(\"lora_r\", 1, 64),#\n",
        "    \"lora_dropout\": trial.suggest_float(\"lora_dropout\", EPSILON, 1, log=True),#\n",
        "    # OPTIM KWARGS\n",
        "    \"optim_arg\": OptimArg(\n",
        "      name=\"AdamW\",\n",
        "      args={\n",
        "        \"lr\": trial.suggest_float(\"weight_decay\", 1e-20, 1e-1, log=True), #1e-8\n",
        "        \"betas\": (beta1, beta2), #(0.9, 0.999)\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-4, 1e-1, log=True), #1e-2\n",
        "      },\n",
        "    )#,\n",
        "  }\n",
        "\n",
        "  trainer.update(**params)\n",
        "  _, res = trainer.train()\n",
        "  score = res[\"scores\"][\"microf1\"]\n",
        "  logger.info(f\"{trial.number}) {params} Score: {score}\")\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "96o4Cfe58tSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "awJ_qzq7JZWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "study = optuna.create_study(direction='maximize') # sampler=TPESampler() by default and maximize f1 score"
      ],
      "metadata": {
        "id": "rg-mGgo17w_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=15) # about 1 hour"
      ],
      "metadata": {
        "id": "dhub_YXo8dfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params, study.best_value"
      ],
      "metadata": {
        "id": "Tq0K_TW814Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMIbVrthzwaf"
      },
      "outputs": [],
      "source": [
        "#%tensorboard --logdir=runs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}